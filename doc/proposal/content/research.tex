\section{Research Topic Summary}
\label{section:research}

\subsection{Data sets introduction}

There are two data sets used for the project. The first one \cite{ThomasDavidson.2020} uses data from the \textit{Twitter API}\footnote{https://github.com/t-davidson/hate-speech-and-offensive-language}. It consists of a sample of around 25k tweets that were identified as hate speech based on a previously composed hate speech lexicon without regarding context information. Subsequently, each document in the corpus got labeled with one of the three categories \textit{hate speech}, \textit{offensive language} or \textit{neutral}. Therefore the data set follows a classical ternary classification style. The workers were instructed to follow predefined definitions of each category and to take context information into consideration. Each tweet was assessed and labeled by three or more workers. The majority of tweets were classified as offensive language (76\% at 2/3, 53\% at 3/3), only 5\% were coded as hate speech. The data is provided offline as a CSV or pickle file. 

The second data set uses data from the \textit{White Supremacy Forum} \cite{OnadeGibert.2020}\footnote{https://github.com/Vicomtech/hate-speech-dataset}. One document represents a sentence that is according to binary classification either labeled as hate or no hate. In total, 1.119 sentences containing hate and 8.537 sentences being non-hate are provided. Once again the documents were labeled manually by human actors following previously specified guidelines, on request additional context information were provided. The documents are given offline as normal text files with annotations stored in a CSV file. 

Both data sets are stated to be balanced, multiple documents cannot be traced back to a single user. In case of imbalanced distributions classifications can be less performant and accurate \cite{Oriola.2020}.

\subsection{Feature extraction}

There are two main approaches for detecting hate speech, either using statistical and probabilistic methods from a conventional machine learning background or using deep learning based approaches. One main difference between the two approaches is the process of feature extraction. In deep learning approaches the used features are learned automatically, whereas classical text analytics techniques require a manual feature extraction process.

The following list provides an overview of possible features and their technical methods from the area of text analytics. The list is clustered into the four categories based on Watanabe, Bouazizi and Ohtsuki \cite{Watanabe.2018}. The possible text analytics approaches are derived from the literature review of Fortuna and Nunes \cite{Fortuna.2018}.
\begin{itemize}
	\item \textbf{Sentiment-based features}: Is the tweet rather positive or negative? \newline
	% allow us to extract the polarity of the tweet
	Text analytics approaches: Dictionaries, Rule Based Approaches, Ob\-jec\-ti\-vi\-ty-Subjectivity of the Language, Declarations of Superiority of the Ingroup \cite{Fortuna.2018}
	\item \textbf{Semantic features}: Which parts of the tweet are emphasized? \newline
	% allow us to find any emphasized expression
	Text analytics approaches: TF-IDF, Part-of-speech, Profanity Windows, Lexical Syntactic Feature-based, Topic Classification, Template Based Strategy, Word Sense Disambiguation Techniques, Othering Language \cite{Fortuna.2018}
	\item \textbf{Unigram features}: Are there any specific words marking hate speech? \newline
	% allow us to detect any explicit form of hate speech
	Text analytics approaches: N-grams, Bag-of-words \cite{Fortuna.2018}
	\item \textbf{Pattern features}: Are there any specific patterns marking hate speech? \newline
	% allow the identification of any longer or implicit forms of hate speech
	Text analytics approaches: Part-of-speech, Dictionaries, Typed De\-pen\-den\-cies,Word Embeddings \cite{Fortuna.2018}
\end{itemize}


\subsection{Conventional machine learning approaches}

Current research in the area of hate speech detection is often built upon deep learning techniques. Exemplary therefore are the works of Roy et al. \cite{Roy.2020}, Setyadi, Nasrun and Setianingsih \cite{NabiilaAdaniSetyadi.2018} and Kapil, Ekbal and Das \cite{Kapil.2020}, to name just a few examples. Among other metrics one can evaluate the success of a choosen approach by considering the accuracy. Roy et al. \cite{Roy.2020} have reached accuracies of up to 95\% for detecting whether a tweet is hateful or not.

Nevertheless classical machine learning techniques such as Support Vector Machines combined with text analytics methods still are a valid and well functioning approach.
Watanabe, Bouazizi and Ohtsuki \cite{Watanabe.2018} leveraged a decision tree and reached an accuracy of 87.4\%. In another paper Oriola and Kotz√© \cite{Oriola.2020} have shown that optimized gradient boosting with word n-gram can achieve a true positive rate of 86.7\%. Gaydhani, Doma, Kendre and Bhagwat \cite{AdityaGaydhani.2018} even evaluated various machine learning models based on n-grams and their according TF-IDF values and achieved an accuracy of 95.6\%, which can definitely compete with modern deep learning approaches and can keep up with these to a certain point. Other relevant work was documented on GitHub\footnote{https://github.com/fidsusj/HateSpeechDetection}. 

When being restricted to a binary classification model one can easily use the concepts that will be proven by this work and apply it to other binary classification tasks like spam detection.
