\section{Analysis} \label{analysis}

For answering our research question, whether classical Machine Learning methods combined with suitable features can outperform neural network based approaches, the following results were achieved:
\begin{itemize}
	\item Investigation of feature importances (chapter \ref{ch:experimentDa})
	\item Hate speech statistics (chapter \ref{ch:experimentDb})
	\item Comparison of the classifier results (classical Machine Learning methods vs neural network based approaches) (chapter \ref{ch:experimentDc})
	\item Oversampled and undersampled datasets (chapter \ref{ch:experimentDd})
\end{itemize}

\subsection{Feature importances}
\label{ch:experimentDa}

\begin{table}[hbt!]
	\caption{Feature importance scores per classifier}
	\label{Tab:featureimportances}
	\begin{tabular}{|p{0.3\textwidth}|p{0.17\textwidth}|p{0.17\textwidth}|p{0.17\textwidth}|p{0.17\textwidth}|}
		\hline
		\textbf{feature}     & \textbf{Decision Tree} & \textbf{Random Forest} & \textbf{SVM} & \textbf{Logistic Regression} \\ \hline
		Unigrams      		 & \gradient{0.096833} & \gradient{0.110481} & \gradient{-0.116233} & \gradient{-0.340829} \\ \hline
		Bigrams       		 & \gradient{0.010989} & \gradient{0.030529} & \gradient{-0.163003} & \gradient{-0.288648} \\ \hline
		Trigrams       		 & \gradient{0.015111} & \gradient{0.041289} & \gradient{-1.389380} & \gradient{-2.009609} \\ \hline
		Hateful Words        & \gradient{0.165297} & \gradient{0.238533} & \gradient{-0.281878} & \gradient{-0.550001} \\ \hline
		Neutral Words        & \gradient{0.078015} & \gradient{0.060794} & \gradient{-0.092602} & \gradient{0.211546} \\ \hline
		Exclamation Marks    & \gradient{0.015078} & \gradient{0.014925} & \gradient{-0.16408} & \gradient{-0.39866} \\ \hline
		Question Marks       & \gradient{0.015837} & \gradient{0.012568} & \gradient{0.077828} & \gradient{0.034885} \\ \hline
		Full Stop Marks      & \gradient{0.050472} & \gradient{0.039024} & \gradient{0.015701} & \gradient{0.018517} \\ \hline
		Interjections        & \gradient{0.001223} & \gradient{0.002545} & \gradient{0.084760} & \gradient{-0.255209} \\ \hline
		All Caps Words       & \gradient{0.030842} & \gradient{0.025809} & \gradient{0.036716} & \gradient{0.119023} \\ \hline
		Quotation Marks      & \gradient{0.007124} & \gradient{0.011336} & \gradient{-0.083534} & \gradient{-0.197908} \\ \hline
		Words Total          & \gradient{0.176739} & \gradient{0.107484} & \gradient{0.023188} & \gradient{0.048423} \\ \hline
		Laughing Expressions & \gradient{0.002860} & \gradient{0.006506} & \gradient{-0.172888} & \gradient{-0.150478} \\ \hline
		Pattern Count        & \gradient{0.094512} & \gradient{0.056026} & \gradient{0.005469} & \gradient{-0.017724} \\ \hline
		Topic       		 & \gradient{0.030162} & \gradient{0.012664} & \gradient{-0.024878} & \gradient{0.002830} \\ \hline
		Sentiment     	     & \gradient{0.208907} & \gradient{0.229479} & \gradient{0.447129} & \gradient{1.180669} \\ \hline
	\end{tabular}
\end{table}

\subsection{Hate speech statistics}
\label{ch:experimentDb}

After extracting all the features, we had a closer look at them to identify which features are characteristic for hate speech.
Firstly, the semantic features in general do not signify whether a post is hate speech or not. Neither the number of exclamation marks, question marks, full stop marks, interjections or all caps words show any sign of signifying hate speech. These features are evenly distributed regarding hate speech versus non-hate speech.
The only semantic features which indicate hate speech are the number of words and - to a very small degree - the number of laughing expressions.
As already mentioned in \autoref{sec:data_analysis} the more words a post consists of, the likelier it is to be classified as hate speech (illustrated in \autoref{fig:wordclouds}).
Although, there are only very few laughing expressions identified per post (most do not contain any), there is a tendency for hate speech posts to contain more laughing expressions, such as \enquote{haha}, \enquote{lol} or similar.

Slightly more telling is the topic feature we trained using LDA with only 2 topics. It seems to have somewhat trained to classify into hate and non-hate - as we hoped. The hate speech posts are more likely to be classified as topic 0 than non-hate speech posts. But this difference is not really significant.

A more interesting and characteristic feature seems to be sentiment-based. As described, we extracted a sentiment-score (polarity) for each post using vader and this clearly differentiates between hate speech and non-hate speech, as shown in \autoref{fig:statistics_sentiment}.
This shows, that a negative sentiment-score indicates a post being rather likely to contain hate speech. The more positive the sentiment-score is, the less probable it is classified as hate speech.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/statistics_sentiment.png}
	\caption{Normalized distribution of sentiment score for hate speech vs. non-hate speech}
	\label{fig:statistics_sentiment}
\end{figure}

Further meaningful features were found using a dictionary approach by using the training data to generate a dictionary for hate speech and neutral words. The number of hateful words is distributed such that hate speech posts contain significantly more, whereas the number of neutral words does not differ much.
Examples for the most common hateful words found in hate speech posts are \enquote{fag}, \enquote{bitch}, \enquote{ass} or \enquote{nigga}. These words basically did not occur in non-hate speech posts. The most common neutral words are less informative, as the suffixes \enquote{ll} and \enquote{ve} are the most common ones for hate speech and non-hate speech posts.

Furthermore, we had an extensive look at unigrams, bigrams and trigrams for hate speech and these are significantly overrepresented in hate speech posts compared to non-hate speech posts. This especially holds true for the unigrams such as \enquote{white} which appears in 15\% of hate speech posts or \enquote{not} appearing in 9\% of hate speech posts. Both of these appear only half as often in non-hate speech posts.
The identified bigrams only show up in a very small percentage of posts, but significantly less in non-hate speech posts. Most common bigrams for hate speech are \enquote{white trash}, \enquote{look like} and \enquote{ass nigga}.

Lastly, the feature pattern-count can somewhat indicate hate speech, as the mean amount is higher for hate speech compared to non-hate speech. As one can see in \autoref{fig:statistics_pattern_count} hate speech tends to contain more patterns (which of course were trained by using the hate speech data).

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/statistics_pattern_count.png}
	\caption{Boxplots comparing the number of patterns occurring in hate speech vs. non-hate speech}
	\label{fig:statistics_pattern_count}
\end{figure}

But maybe it would be worthwhile to have a closer look at the patterns and adjust them for some future work. Because when we look at single patterns, some occur more often in hate speech and some more often in non-hate speech (in our dataset). For example the pattern \enquote{adjective, noun (JJ, NN)} occurs in 56\% of hate speech and in 48\% of non-hate speech, whereas the pattern \enquote{determiner, noun} occurs in 5\% less hate speech posts than non-hate speech posts. So a more thorough analysis of these patterns could benefit this feature a lot, maybe by using individual features for each pattern. For example the biggest difference in occurrences is achieved by the pattern \enquote{personal pronoun, non-3rd person singular present verb (PRP, VBP)} with a 10\% difference.

\subsection{Comparison of the classifier results}
\label{ch:experimentDc}

All classifiers perform about equally well. And the achieved results are good with about 93\% for the F1-score. In the unbalanced case the conventional Machine Learning methods can definitely keep up with the neural network baseline. 

\subsection{Oversampled and undersampled datasets}
\label{ch:experimentDd}

In the undersampled case all conventional Machine Learning methods perform about equally well. Compared to the results from unbalanced dataset the conventional classifiers perform worse. In contrast the neural network baseline can keep up with the results from the unbalanced case. So in this undersampled case the conventional Machine Learning methods cannot keep up with the neural network baseline.

In the oversampled case the results are located between the undersampled and the unbalanced case. An outstanding result is the Random Forest, which performs better then the other conventional Machine Learning methods. As already mentioned SMOTE is not able to generate new textual features for generating an oversampled dataset for the neural network baseline. That is why these results could not be measured.
