\section{Introduction}

One current research area in the field of text analytics is hate speech detection. Many approaches from the recent past take use of neural network architectures to deal with such classification problems. One of the most common approaches is to use uni- and bidirectional long short-term memory (LSTM) networks, a recurrent neural network architecture that can process input of arbitrary length and remembers context information \cite{Dorris2020, Syam2019, Saksesi2018}. The paper \cite{Founta2019} states, that even a simple gated recurrent unit (GRU) architecture can perform as good as more complex units. \cite{Saleh2020} repurposes the famous bidirectional encoder representations from transformers (BERT) language model to perform clas\-si\-fi\-ca\-tion tasks for hate speech detection. Besides that, other approaches use con\-vo\-lu\-tional neural networks (CNNs) to extract typical hate speech patterns \cite{Badjatiya2017, Roy2020, Kapil2020} or even deep belief network algorithms \cite{Muhammad2020}. Using neural network approaches means to automatically learn representative features for the classification task. On the other hand, the papers introduced in \autoref{related_work} use a different approach by solving the classification task with manually extracted features. Nevertheless, none of the papers combines the different achievements of such recent research and compares it to a baseline neural network architecture, which is what this work is dedicated to.

After a definition of the term \enquote{hate speech} in \autoref{approach} different classifiers will be trained on a holistic, hand-crafted feature set based on recent pub\-li\-ca\-tions in the field of hate speech detection. The task includes building and pre\-pro\-cess\-ing a training corpus as well as introducing and explaining the different kinds of features. How well the different classifiers perform compared to a neural network approach as a baseline and several statictical insights into typical hate speech artifacts will be presented in \autoref{results}. The results of this work in \autoref{analysis} should show which features work best for which classifier and which problems can be addressed with conventional machine learning methods and which not as opposed to neural network approaches. A summary over the achievements earned will be drawn in \autoref{conclusion}.