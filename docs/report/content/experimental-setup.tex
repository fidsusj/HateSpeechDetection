\section{Experimental setup and results} \label{results}

\subsection{Data}

There are two data sets used for the project. The first one uses data from the \textit{Twitter API} \cite{ThomasDavidson2020}\footnote{https://github.com/t-davidson/hate-speech-and-offensive-language}. It consists of a sample of around 25k tweets that were identified as hate speech based on a previously composed hate speech lexicon without regarding context information. Subsequently, each document in the corpus got labeled with one of the three categories \textit{hate speech}, \textit{offensive language} or \textit{neutral}. Therefore the data set follows a classical ternary clas\-si\-fi\-ca\-tion style. The workers were instructed to follow predefined definitions of each category and to take context information into consideration. Each tweet was assessed and labeled by three or more workers. The majority of tweets were classified as offensive language (76\% at 2/3, 53\% at 3/3), only 5\% were coded as hate speech. The data is provided offline as a CSV or pickle file. 

The second data set uses data from the \textit{White Supremacy Forum} \cite{DeGibert2020}\footnote{https://github.com/Vicomtech/hate-speech-dataset}. One document represents a sentence that is either labeled as hate or not hate. In total, 1.119 sentences containing hate and 8.537 sentences being non-hate are provided. Once again the documents were labeled manually by human actors following previously specified guidelines, on request additional context information was provided. The documents are given offline as normal text files with annotations stored in a separate CSV file. 

\subsubsection{Data preparation}

To prepare a central dataset, both single datasets had to be transformed into a common format. For the central dataset only the class and the text content of each tweet respectively each forum contribution was considered.

The first dataset \citetitle{ThomasDavidson2020} was entirely given as a .csv file and contains 25.297 tweets, that were either labeled as hate speech, offensive language or neither of both. To determine the right label three independent evaluators classified each tweet, the final label got assigned by the majority vote. As for the first approach, one is only interested in hate speech and neutral tweet classification, all offensive language documents in the dataset were dropped. Some tweets were retweets that were commented additionally by a user. As it could not be distinguished whether the original tweet or the retweet contains hate speech, these documents were filtered out as well. An example is shown below:

\begin{quote}
    """@jaimescudi\_: ""@Tonybthrz\_: ""@jaimescudi\_: I swear if oomf try talking to me tomorrow.."" @"" @BarackObama"" pussy"
\end{quote}

The original tweets can be found in between the ""..."". Same goes for tweets that cite other users without using the retweet option.

\vspace{0.5cm}

The second dataset \citetitle{DeGibert2020} was not entirely given as a .csv file. Only the document annotations were given in a .csv file, all forum contributions were stored in separate .txt files. Only documents which could not be assigned to a single class (label "idk/skip") or referred to other documents (label "relation") were dropped.

The resulting common dataset was stored in a .csv file. It contains 2.491 hate speech documents and 13.336 non hate speech documents. The dropped offensive language documents make up 17.505 instances. In case the classification results are too poor, additional 2.818 offensive language documents can be added that were labeled as hate speech by one evaluator.

\subsubsection{Corpus building}

The common dataset is loaded from the .csv file into a pandas dataframe. After doing basic preprocessing like removing emojis and other irrelevant characters, spacy is used to build a tokenized corpus. The language model that spacy brings decides about stop word, punctuation and white space removal. No hard coded logic or stop word lists are used in this process. This keeps URLs or other tokens including punctuation as one token. Furthermore no stemming was applied to the tokens, instead lemmatization was used as one can in this case later on use pre-trained word embeddings from i.e. Word2Vec. Furthermore tokenization works better using the lemmas instead of word stems (e.g. We'll becomes ["we","will"] and not ["we", "'ll"].

\subsubsection{Data analysis} \label{sec:data_analysis}

As already mentioned, the acquired dataset is an imbalanced one, which can lead to a decrease in performance and accuracy with machine learning classification. As a comparison the paper \cite{Oriola2020} also recognizes the class imbalance and tries to reduce it by applying a synthetic minority oversampling technique called SMOTE \cite{Chawla2011}. In general there are a few possibilities to tackle the challenge of unbalanced classes:

\begin{itemize}
    \item changing the performance metric (e.g. F1-score instead of accuracy)
    \item undersampling, i.e. deleting instances from the over-represented class
    \item oversampling, i.e. adding copies of instances from the under-represented class
    \item generating synthetic samples (e.g by using SMOTE)
\end{itemize}

In our experiments we chose to try the different approaches and applied a simple undersampling, as well as an oversampling using SMOTE. Additionally, we used the imbalanced dataset to train a classifier to compare how this affects the performance.

\vspace{1cm}

In further analysis of the data, we had a look at the length of hate speech posts versus non-hate speech posts. This can be seen in \autoref{fig:post_length_density_distribution}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/post_length_density_distribution.png}
    \caption{Density distribution of the length of a post (tweet or forum post)}
    \label{fig:post_length_density_distribution}
\end{figure}

Here one can see, that the hate speech posts contain more words (tokens before cleaning) than non-hate speech posts. In average a hate speech post contains 18.18 words, whereas a non-hate speech post only contains 15.85 words. Unlike expected, the hate speech posts are longer than the non-hate speech posts.

A more interesting look at the data are the most commonly used words per class. As can be seen in the word clouds in \autoref{fig:wordclouds} there are some obvious differences, such that the hate speech posts use words like \enquote{bitch}, \enquote{faggot} or \enquote{nigga}. But interestingly enough, the non-hate speech posts also often consist of the words \enquote{trash} or \enquote{white}.

\begin{figure}[ht]
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Wordcloud-HateSpeech-tokens.png}
        \caption{hate speech}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Wordcloud-Non-HateSpeech-tokens.png}
        \caption{non-hate speech}
    \end{subfigure}
    \hfill
    \caption{Word clouds}
    \label{fig:wordclouds}
\end{figure}

For a better insight with what data we are dealing, a few examples are shown in the following.

\noindent
Examples for non-hate speech (neutral sentences):
\begin{itemize}
    \item "billy that guy would nt leave me alone so i gave him the trudeau salute"
    \item "this is after a famous incident of former prime minister pierre trudeau who gave the finger to a group of protesters who were yelling antifrench sayings at him"
    \item "askdems arent you embarrassed that charlie rangel remains in your caucus"
\end{itemize}

\noindent
Examples for hate speech:
\begin{itemize}
    \item "california is full of white trash"
    \item "and yes they will steal anything from whites because they think whites owe them something so it s ok to steal"
    \item "why white people used to say that sex was a sin used to be a mystery to me until i saw the children of browns and mixed race children popping up all around me"
\end{itemize}

One can clearly see the hate expressed in the hate speech examples and see their discriminating nature.


\subsection{Evaluation method}

As part of the approach chapter \ref{ch:approachF} possible evaluation metrics were already introduced. Our evaluation is built upon existing quantitative metrics. Especially the F1 score is relevant, because it takes the precision (risk of censorship) and the recall (risk of ineffectiveness) into account. The f1 score is defined as follows:
\begin{equation}
F1-score = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}
\end{equation}

\subsection{Experimental details}

The optimal hyperparameters of the conventional Machine Learning methods are learned automatically as part of the pipeline. Nevertheless the learned optimal hyperparameters are listed in this chapter to enable the replication of our results. For readability only the hyperparameters, which differ from the default values are listed.

Optimal hyperparameters for the unbalanced dataset:
\begin{itemize}
	\item Decision Tree: max\_leaf\_nodes=15, min\_samples\_leaf=10
	\item Random Forest: criterion='entropy', max\_depth=10, max\_features='log2'
	\item SVM: kernel='linear'
	\item Logistic Regression: C=1.2, max\_iter=1500
\end{itemize}

Optimal hyperparameters for the undersampled dataset:
\begin{itemize}
	\item Decision Tree: class\_weight='balanced', criterion='entropy', max\_depth=10, max\_leaf\_nodes=15, min\_samples\_split=40
	\item Random Forest: max\_depth=10, max\_features='sqrt'
	\item SVM: kernel='linear'
	\item Logistic Regression: C=1.2, max\_iter=1500, solver='saga'
\end{itemize}

Optimal hyperparameters for the oversampled dataset:
\begin{itemize}
	\item Decision Tree: class\_weight='balanced', criterion='entropy'
	\item Random Forest: criterion='entropy', max\_features='sqrt'
	\item SVM: all default parameters
	\item Logistic Regression: C=1.2, max\_iter=1500
\end{itemize}


\subsection{Results}

For answering our research question, whether classical Machine Learning methods combined with suitable features can outperform neural network based approaches, following results were achieved:
\begin{itemize}
	\item Investigation of feature importances (chapter \ref{ch:experimentDa})
	\item Hate speech statistics (chapter \ref{ch:experimentDb})
	\item Comparison of the classifier results (classical Machine Learning methods vs neural network based approaches) (chapter \ref{ch:experimentDc})
	\item Oversampled and undersampled datasets (chapter \ref{ch:experimentDd})
\end{itemize}

\subsubsection{Feature importances}
\label{ch:experimentDa}


\subsubsection{Hate speech statistics}
\label{ch:experimentDb}

After extracting all the features, we had a closer look at them to identify which features are characteristic for hate speech.
Firstly, the semantic features in general do not signify whether a post is hate speech or not. Neither the number of exclamation marks, question marks, full stop marks, interjections or all caps words show any sign of signifying hate speech. These features are evenly distributed regarding hate speech versus non-hate speech.
The only semantic features which indicate hate speech are the number of words and - to a very small degree - the number of laughing expressions.
As already mentioned in \autoref{sec:data_analysis} the more words a post consists of, the likelier it is to be classified as hate speech (illustrated in \autoref{fig:wordclouds}).
Although, there are only very few laughing expressions identified per post (most do not contain any), there is a tendency for hate speech posts to contain more laughing expressions, such as \enquote{haha}, \enquote{lol} or similar.

Slightly more telling is the topic feature we trained using LDA with only 2 topics. It seems to have somewhat trained to classify into hate and non-hate - as we hoped. The hate speech posts are more likely to be classified as topic 0 than non-hate speech posts. But this difference is not really significant.

A more interesting and characteristic feature seems to be sentiment-based. As described, we extracted a sentiment-score (polarity) for each post using vader and this clearly differentiates between hate speech and non-hate speech, as shown in \autoref{fig:statistics_sentiment}.
This shows, that a negative sentiment-score indicates a post being rather likely to contain hate speech. The more positive the sentiment-score is, the less probable it is classified as hate speech.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/statistics_sentiment.png}
	\caption{Normalized distribution of sentiment score for hate speech vs. non-hate speech}
	\label{fig:statistics_sentiment}
\end{figure}

Further meaningful features were found using a dictionary approach by using the training data to generate a dictionary for hate speech and neutral words. The number of hateful words is distributed such that hate speech posts contain significantly more, whereas the number of neutral words does not differ much.
Examples for the most common hateful words found in hate speech posts are \enquote{fag}, \enquote{bitch}, \enquote{ass} or \enquote{nigga}. These words basically did not occur in non-hate speech posts. The most common neutral words are less informative, as the suffixes \enquote{ll} and \enquote{ve} are the most common ones for hate speech and non-hate speech posts.

Furthermore, we had an extensive look at unigrams, bigrams and trigrams for hate speech and these are significantly overrepresented in hate speech posts compared to non-hate speech posts. This especially holds true for the unigrams such as \enquote{white} which appears in 15\% of hate speech posts or \enquote{not} appearing in 9\% of hate speech posts. Both of these appear only half as often in non-hate speech posts.
The identified bigrams only show up in a very small percentage of posts, but significantly less in non-hate speech posts. Most common bigrams for hate speech are \enquote{white trash}, \enquote{look like} and \enquote{ass nigga}.

Lastly, the feature pattern-count can somewhat indicate hate speech, as the mean amount is higher for hate speech compared to non-hate speech. As one can see in \autoref{fig:statistics_pattern_count} hate speech tends to contain more patterns (which of course were trained by using the hate speech data).

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/statistics_pattern_count.png}
	\caption{Boxplots comparing the number of patterns occurring in hate speech vs. non-hate speech}
	\label{fig:statistics_pattern_count}
\end{figure}

But maybe it would be worthwhile to have a closer look at the patterns and adjust them for some future work. Because when we look at single patterns, some occur more often in hate speech and some more often in non-hate speech (in our dataset). For example the pattern \enquote{adjective, noun (JJ, NN)} occurs in 56\% of hate speech and in 48\% of non-hate speech, whereas the pattern \enquote{determiner, noun} occurs in 5\% less hate speech posts than non-hate speech posts. So a more thorough analysis of these patterns could benefit this feature a lot, maybe by using individual features for each pattern. For example the biggest difference in occurrences is achieved by the pattern \enquote{personal pronoun, non-3rd person singular present verb (PRP, VBP)} with a 10\% difference.


\subsubsection{Comparison of the classifier results}
\label{ch:experimentDc}

Table \ref{Tab:unchanged} shows the performance metrices of the classifiers for the unbalanced dataset. All classifiers perform about equally well. And the achieved results are good with about 93\% for the F1-score. So in this unbalanced case the conventional Machine Learning methods can definitely keep up with the neural network baseline. 

\begin{table}[hbt!]
	\caption{Classifier results for unbalanced dataset}
	\label{Tab:unchanged}
	\begin{tabular}{|p{0.3\textwidth}|p{0.13\textwidth}|p{0.13\textwidth}|p{0.13\textwidth}|p{0.13\textwidth}|}
		\hline
		\textbf{classifier} & \textbf{precision} & \textbf{recall} & \textbf{accuracy} & \textbf{F1} \\ \hline
		Decision Tree       & \gradient{0.8756} & \gradient{0.9821} & \gradient{0.8671} & \gradient{0.9258} \\ \hline
		Random Forest       & \gradient{0.8809} & \gradient{0.9894} & \gradient{0.8782} & \gradient{0.9320} \\ \hline
		SVM                 & \gradient{0.8697} & \gradient{0.9927} & \gradient{0.8684} & \gradient{0.9272} \\ \hline
		Logistic Regression & \gradient{0.8831} & \gradient{0.9832} & \gradient{0.8760} & \gradient{0.9305} \\ \hline
		LSTM                & \gradient{0.9219} & \gradient{0.9567} & \gradient{0.8950} & \gradient{0.9390} \\ \hline
	\end{tabular}
\end{table}

\subsubsection{Oversampled and undersampled datasets}
\label{ch:experimentDd}

Table \ref{Tab:undersampled} shows the performance metrices of the classifiers for the undersampled dataset and table \ref{Tab:oversampled} for the oversampled dataset. 

In the undersampled case all conventional Machine Learning methods perform about equally well. Compared to the results from unbalanced dataset the conventional classifiers perform worse. In contrast the neural network baseline can keep up with the results from the unbalanced case. So in this undersampled case the conventional Machine Learning methods cannot keep up with the neural network baseline.

\begin{table}[hbt!]
	\caption{Classifier results for undersampled dataset}
	\label{Tab:undersampled}
	\begin{tabular}{|p{0.3\textwidth}|p{0.13\textwidth}|p{0.13\textwidth}|p{0.13\textwidth}|p{0.13\textwidth}|}
		\hline
		\textbf{classifier} & \textbf{precision} & \textbf{recall} & \textbf{accuracy} & \textbf{F1} \\ \hline
		Decision Tree       & \gradient{0.7202} & \gradient{0.7710} & \gradient{0.7431} & \gradient{0.7448} \\ \hline
		Random Forest       & \gradient{0.7261} & \gradient{0.8043} & \gradient{0.7573} & \gradient{0.7632} \\ \hline
		SVM                 & \gradient{0.7193} & \gradient{0.8375} & \gradient{0.7621} & \gradient{0.7739} \\ \hline
		Logistic Regression & \gradient{0.7246} & \gradient{0.8238} & \gradient{0.7621} & \gradient{0.7710} \\ \hline
		LSTM                & \gradient{0.9219} & \gradient{0.9567} & \gradient{0.8950} & \gradient{0.9390} \\ \hline
	\end{tabular}
\end{table}

In the oversampled case the results are located between the undersampled and the unbalanced case. 
An outstanding result is the Random Forest, which performs better then the other conventional Machine Learning methods. As already mentioned the SMOTE is not able to generate new textual features for generating an oversampled dataset for the neural network baseline. That is why these results are not measured.

\begin{table}[hbt!]
	\caption{Classifier results for oversampled dataset}
	\label{Tab:oversampled}
	\begin{tabular}{|p{0.3\textwidth}|p{0.13\textwidth}|p{0.13\textwidth}|p{0.13\textwidth}|p{0.13\textwidth}|}
		\hline
		\textbf{classifier} & \textbf{precision} & \textbf{recall} & \textbf{accuracy} & \textbf{F1} \\ \hline
		Decision Tree       & \gradient{0.7973} & \gradient{0.7919} & \gradient{0.7924} & \gradient{0.7946} \\ \hline
		Random Forest       & \gradient{0.8844} & \gradient{0.8557} & \gradient{0.8701} & \gradient{0.8698} \\ \hline
		SVM                 & \gradient{0.7648} & \gradient{0.8081} & \gradient{0.7767} & \gradient{0.7859} \\ \hline
		Logistic Regression & \gradient{0.7573} & \gradient{0.8081} & \gradient{0.7713} & \gradient{0.7819} \\ \hline
		LSTM                & \multicolumn{4}{|c|}{not measured} \\ \hline
	\end{tabular}
\end{table}