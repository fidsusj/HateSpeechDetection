{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: “Text Mining with Simpsons ”\n",
    "Due: Monday 2pm, December 14, 2020, via Moodle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission guidelines\n",
    "\n",
    "- Solutions need to be uploaded as a single Jupyter notebook. You will find many provided codes in the notebook, your task is to fill in the missing cells.\n",
    "- For the written solution use LaTeX in markdown inside the same notebook. Do *not* hand in a seperate file for it.\n",
    "- Download the .zip file containing the dataset but do *not* upload it with your solution.\n",
    "- It is sufficient if one person per group uploads the solution to Moodle, but make sure that the names of\n",
    "all team members are given on the PDF and in the source code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Simpson show is the world’s longest-running animated sitcom. The show revolves around the lives of the Simpson family, which consists of Homer, Marge, Bart, Lisa, and the little Maggi.\n",
    "For this notebook, you should download the dataset that contains the characters, locations, episode details, and script lines for approximately 600 Simpsons episodes, dating back to 1989.\n",
    "In the following exercises, we will explore the data and use visualizations to gain some insight into the show. The Zip file alongside this notebook should contain the following files:\n",
    "\n",
    "` simpsons_characters.csv, simpsons_locations.csv, simpsons_script_lines.csv, simpsons_episodes.csv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take some time and familiarize yourself with the data. `simpsons_characters.csv` contains the character-names and their gender.\n",
    "`simpsons_locations.csv` contains information about locations in the shows,\n",
    "and `simpsons_episodes.csv` has information about each episode including their title, rating, and views.\n",
    "Finally, `simpsons_script_lines.csv` lists all the lines in the show, who said the line and where it was said. Be aware that this data set is not cleaned and, hence, includes faulty data as well as inconsistent data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the needed packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from os import path\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the data and get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1:  Important Characters (3 + 2 + 2 + 4 = 11 point)\n",
    "### Sub-Task 1: \n",
    "Load the four datasets in the respective variables using pandas (pd),\n",
    "make sure to set the type of the `character_id` to integer and drop the line that has the non-numeric character ID `142024`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_characters = ### Your code ####\n",
    "df_locations = ### Your code ####\n",
    "df_script = ### Your code ####\n",
    "df_episodes = ### Your code ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using panda's `.head()` function look at the top rows of each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Your Code####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the important characters of the show. To start we detect the main characters by the number of spoken words/lines throughout the show. \n",
    "Extract top 10 characters based on the number of spoken words (word count) as well as the 10 most prominent characters regarding the number of lines they head during the show.\n",
    "Compare both results by drawing two bar plots: For the first plot, the x-axis should show the name of the top 10 characters with regard to the word count and the y-axis should display the number of spoken words.\n",
    "In the second plot, you should do the same but this time considering the number of lines for each character (,i.e., the characters displayed on the x-axis can be different for the first and second bar plot). You might notice that there is a column with `word_count` in the `scripts` data frame, but cleaning it up might be tricky. To find the sum of all values, first remove the `nan` and any other string characters in that colum, you can use the `conv` function provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def conv(row):\n",
    "    try:\n",
    "        return int(row)\n",
    "    except:\n",
    "        return np.nan\n",
    "df_script[\"word_count\"] = ### Your code ###\n",
    "df_script[\"word_count\"].dropna(inplace=True)\n",
    "top_characters_words = ### Your code ###\n",
    "merge_data_words_sub = ### merge to get the names ###\n",
    "merge_data_words_sub.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_characters_sentences = ### Your code ###\n",
    "merge_data_sentences_sub = ### Your code ###\n",
    "merge_data_sentences_sub.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Draw the word count plot ####\n",
    "#### Hint: look at the plot function from dataframe #### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Draw the sentence count plot ####\n",
    "#### Hint: look at the plot function from dataframe #### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have done the exercise correctly you should see that the top-4 characters of the show (Homer, Marge, Bart, and Lisa Simpson) have the most dialogs (lines) in the show but their distribution differs when we look at the word count.\n",
    "Seems like some characters speak in long sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Task 2: Common Words and Word Cloud \n",
    "Let's examine the dialogues and look at the top 20 common words, draw a bar plot in the same way as above to show the frequencies.\n",
    "To generate a meaningful output use the preprocessing pipelines explained in the lecture: \n",
    "- use regex to remove non-alphabetic characters and also remove `'` since the tokenizer will treat it as a new token (anything that is not a number or alphabet including punctuations)\n",
    "- lowercase all words\n",
    "- remove stopwords based on spaCy's stopword list\n",
    "- tokenize the `spoken_words` (remove single characters produced by the tokenizer)\n",
    "- perform stemming \n",
    " \n",
    "In this exercise, we require you to use SpaCy for all language processing steps except for stemming.\n",
    "This is due to the lack of a stemmer in SpaCy's library that only provides a tool for lemmatization.\n",
    "Hence, for Stemming we are going to use one of the stemmers provided by NLTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import re \n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.### load en_core_web_sm and disable the tagger, parser and ner.\n",
    "stopwords = ### load the list of stopwords from spacy for the English language\n",
    "stemmer = ### initialize the stemmer from NLTK\n",
    "\n",
    "df_script[\"spoken_words\"] = df_script[\"spoken_words\"]. ### Your code to lower case and remove non-alphabetic characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "#### Count the words in the scripts and print out the most common 20, remove any single character tokens and stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### draw a bar plot with the x axis as the words and the y axis as the frequencys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a word cloud to visualize the frequencies:\n",
    "A word cloud is a graphical representation of frequently used words in the normalized text.\n",
    "The height of each word in this picture is an indication of the frequency of occurrence of the word in the entire text.\n",
    "You will need to install the package `wordcloud`.\n",
    "To achieve a homogeneous output, set the `max_words` to 100 and `max_font_size` to 60.\n",
    "Make sure the same word does not appear more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud( \n",
    "    #### your code ####\n",
    ")\n",
    "    \n",
    "fig = plt.figure(1, figsize=(12, 12))\n",
    "plt.axes(\"off\")\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Task 3: Common Named Entities \n",
    "Apply named entity recognition using SpaCy and generate a word cloud of the top 50\n",
    "named entities that have the type `PERSON`, using the same approach\n",
    "for visualization as above. Take into account that you cannot simply use the\n",
    "output of the previous step and need to undo the stemming. Since the named entity\n",
    "recognition takes quite some time, use only the first `10000` scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = # Load the small English spacy model again, this time with all components enabled\n",
    "\n",
    "for script in df_script.spoken_words.tolist()[:10000]:\n",
    "    ###Your code ##### \n",
    "    \n",
    "fig = plt.figure(1, figsize=(12, 12))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does the named entity recognition work? Do you notice some strange \"people\" in there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 4 : Sophistication of Speech\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take into account the top 10 characters from the first subtask (based on line count), it is interesting to see whether there are big differences in the sophistication of speech or vocabulary size between the characters.\n",
    "To measure this, we turn to the Flesch Reading Ease. Although designed for written text we use it here on spoken words.\n",
    "This measures indicates how difficult a passage in English is to understand and is based on a ranking scale of 0-100, where higher scores indicate material that is easier to read; lower numbers mark passages that are more difficult to read.\n",
    "For more information look at the [wiki page](https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests).\n",
    "\n",
    "$206.835 - 1.015 \\times \\frac{total words}{total sentences} - 84.6 \\times \\frac{total syllables}{total words}$\n",
    "\n",
    "For syllable counts, download `cmudict` from the NLTK library and count the vowel sounds from the Carnegie Mellon Pronouncing Dictionary (cmudict).\n",
    "Then count vowel sounds in each word, if a word is not in the dictionary do not count it in.\n",
    "CMUdict marks each vowel with a stress tag, 0 (unstressed), 1 (stressed), or 2 (secondary stress)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('cmudict')\n",
    "from nltk.corpus import cmudict\n",
    "phoneme_dict = dict(cmudict.entries())\n",
    "nlp = # load the same spacy model again, with tagger, parser and ner disabled.\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "\n",
    "def syllable_counter(word):\n",
    "    ### functionn that counts a syllable in a word \n",
    "\n",
    "def total_sylls(x): \n",
    "    ### function to use with apply method, to count the total number of syllable in line of a script \n",
    "    \n",
    "def sentence_count(x):\n",
    "    ### function to use with apply method, to count the total number of sentences in line of a script \n",
    "\n",
    "df_script['syllable_count'] =\n",
    "df_script['sentence_count'] = \n",
    "top_characters_counts = ## use aggragations to find the word_count, syllable_count and sentence_count per person \n",
    "top_characters_counts['Flesch_readability'] =\n",
    "merge_data_words_sub = ### merge with df_characters to find the character nanmes \n",
    "### draw plot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: TF-IDF ( 3 + 4 = 7 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub Task 1: \n",
    "Despite all the preprocessing, so far the top words we looked at are not so informative.\n",
    "We wish to find out the important words that are spoken by the top characters.\n",
    "For example, the youngest of the family should have \"mom\" as an important word.\n",
    "We looked at the Term Frequency - Inverse Document Frequency (TF-IDF) weighting in the lecture, so let's use that here to detect the most important words per character.\n",
    "In this case, each script line is a document for a certain character who spoke the words.\n",
    "Use `CountVectorizer` and `TfidfTransformers` from scikit-learn, and use the scikit-learn stop word list to remove stop words and remove the words with a frequency less than 5 from the analysis.\n",
    "Then plot the TF-IDF values for the top 10 words for Homer, Marge, Bart, and Lisa Simpson as a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "del df_script['id']\n",
    "df_script = df_script.rename(columns = {'character_id':'id'})  # to merge easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = #merge the dataframes to the characters to get the names and drop nan values\n",
    "\n",
    "for name in ['Homer Simpson', 'Marge Simpson', 'Bart Simpson', 'Lisa Simpson']:\n",
    "    ##### Your code ######\n",
    "    count_vectorized =\n",
    "    transformer = \n",
    "    ### Don't forget that you need the tfidf values for a single word averaged across documents ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did the exercise correctly, \"mom\" and \"dad\" should be among the top words for the childern and \"homer\" should be the top word for Marge, since this is what she calls her husband."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub Task 2: \n",
    "If we consider the spoken words from each character a document of its own, we can generate 4 documents (by concatenation of all dialogs) for Homer, Marge, Bart, and Lisa Simpson, and create document vectors from those.\n",
    "Let's take a look at how the values in these vectors are distributed. Use  `sns.heatmap` from the seaborn package to show the vectors of the 4 documents for the top-20 words (set the `max_features` parameter of the `CountVectorizer` to 20).\n",
    "Compare it with the heatmap of only term frequencies of the documents. Use `fmt='g'` to print out the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(17,6))         \n",
    "sns.heatmap(### Your tfidf weights ###\n",
    "    , annot=True, cbar=False, ax=ax, xticklabels=# top 20 words\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(17,6))         \n",
    "sns.heatmap(### Your count vectorizer weights ###\n",
    "    , annot=True, cbar=False, ax=ax, xticklabels=#top 20 words\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the TF_IDF vectors of the top-20 words, which characters are similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise\n",
    "names = ['Homer Simpson', 'Marge Simpson', 'Bart Simpson', 'Lisa Simpson']\n",
    "cosine= ### Your code ### \n",
    "for i,name in zip(range(4),names):\n",
    "    for j,name2 in zip(range(i+1,4),names[i+1:]):\n",
    "        print('{} to {}: {}'.format(name, name2, ### Your code to compute cosine similarity ###))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Topics ( 4 + 3 = 7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to inspect the topics often discussed in the show. We look at SVD (LSA) and NMF for topic detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub Task1:\n",
    "Use `NMF` from scikit-learn to detect the topics in the scripts, and use the text cleaning steps from the first task:\n",
    "- use regex to remove non-alphabetic characters and also remove `'` since the tokenizer will treat it as a new token (anything that is not a number or alphabet including punctuations)\n",
    "- lowercase all words\n",
    "- remove stopwords based on spaCy's stopword list for English\n",
    "- tokenize the spoken_words (remove single characters produced by the tokenizer) \n",
    "- perform stemming \n",
    "\n",
    "set the `max_features` of  `CountVectorizer` to 10,000 and `random_state` of the `NMF` to 42, `n_components=10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF \n",
    "stopwords = # load spacy's stop word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_topics(components, num_top_words, vocab):\n",
    "    #for each component or topic sorts the row values from large to small and returns the top words and the representation of the topic.\n",
    "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n",
    "    topic_words = ([top_words(t) for t in components])\n",
    "    return [' '.join(t) for t in topic_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(#### Your Code ####\n",
    "porter = #### initlize the stemmer ####\n",
    "df_script.dropna(inplace=True)\n",
    "df_script.spoken_words =#### Your Code ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(#### Your Code #### )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `show_topics()` method to show the top-20 words for the top 2 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your Code ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub Task 2:\n",
    "The SVD algorithm factorizes a matrix into one matrix with orthogonal columns and one with orthogonal rows (along with a diagonal matrix, which contains the relative importance of each factor).\n",
    "Latent Semantic Analysis (LSA) uses SVD. Here we use the 'TruncatedSVD' method from 'sklearn' to look at the topics.\n",
    "This is faster than SVD since we focus only on the largest singluar values.\n",
    "Use the cleaned documents form the substask before. Where `random_state=42`, `n_components=10`, and `n_iter=7`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(#### Your Code ####)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `show_topics` method to show the top-20 words for the top-2 topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_topics(#### Your Code ####)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there is not much to make out of these topics. But topic models also give us document representations, so let's compare Homer, Marge, Bart, and Lisa Simpson based on their SVD vectors.\n",
    "Use the same approach as the TF_IDF Task, but apply the pre-processing steps mentioned in the previous subtask.\n",
    "Notice that if you use the default parameters for the SVD you might encounter a strange shape in the transformed data.\n",
    "This happens because the underlying algorithm is set to 'randomized_svd'.\n",
    "Investigate how to solve this problem and run the algorithm with `n_components=2, n_iter=7, random_state=40`.\n",
    "Hint: You need one extra parameter to overcome this problem.\n",
    "Transform the TF_IDF vectors for the 4 subsets to the SVD and compute the cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = {}\n",
    "for name in ['Homer Simpson', 'Marge Simpson', 'Bart Simpson', 'Lisa Simpson']:\n",
    "    subsets[name] = merged_data[merged_data['name']==name]['normalized_text'].### Your code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorized = CountVectorizer(analyzer='word', max_features=10000)\n",
    "### Your Code ### \n",
    "svd = ### Your Code ###\n",
    "svd.fit(### Your Code ### \n",
    "new_weights = ### Your Code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Homer Simpson', 'Marge Simpson', 'Bart Simpson', 'Lisa Simpson']\n",
    "cosine = ###Compute the cosine similirity###\n",
    "for i, name in zip(range(4),names):\n",
    "    for j, name2 in zip(range(i+1,4),names[i+1:]):\n",
    "        print('{} to {}: {}'.\n",
    "                  format(name,name2,###Cosine Similiarity### ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Mathematical Concepts ( 1 + 2 + 2 = 5 points)\n",
    "Answer the following questions in the notebook, use markdown or latex to fill in the cells for the answers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub Task 1: \n",
    "What is the relationship between PCA and SVD? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub Task 2: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the Singular value decomposition of  $A = \\left[ \\begin{matrix}1&-1\\\\ -2&2\\\\ 2&-2\\end{matrix} \\right]$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub Task 3:\n",
    "\n",
    "Consider the term-document matrix \n",
    "\n",
    "|        | d1 | d2 | d3 | d4 | d5 | d6 |\n",
    "|--------|----|----|----|----|----|----|\n",
    "| bank   | 1  | 2  | 1  | 2  | 1  | 1  |\n",
    "| money  | 0  | 0  | 0  | 2  | 1  | 2  |\n",
    "| river  | 2  | 0  | 2  | 0  | 0  | 0  |\n",
    "| ship   | 2  | 1  | 1  | 0  | 0  | 0  |\n",
    "| water  | 1  | 2  | 2  | 0  | 0  | 0  |\n",
    "| invest | 0  | 0  | 0  | 1  | 2  | 0  |\n",
    "| loan   | 0  | 0  | 0  | 1  | 1  | 1  |\n",
    "\n",
    "Use NumPy to compute the SVD for the matrix and write down the term matrix (U) and document matrix ($V^T$) and the singular values ($\\Sigma$). Compute the following: \n",
    "\n",
    "- zero out everything but the 2 largest singular values of $\\Sigma$ and compute the new term-document matrix.\n",
    "- detect the two topics given the new $\\Sigma$ and show the top-3 words for each. What are these topics? \n",
    "- what is the most similar document to each document, using the cosine similarity and reduced representation? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "A = np.array(### Your Code ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, vh = np.linalg.### Your Code ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reconstruction= ### Your Code ####\n",
    "print(new_reconstruction.round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=['bank', 'money', 'river', 'ship', 'water', 'invest', 'loan']\n",
    "top_words = ### Your Code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise\n",
    "cosine = ### Your Code ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
