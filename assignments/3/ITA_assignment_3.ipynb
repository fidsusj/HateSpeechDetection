{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 : \"Word Vectors and Classification\"\n",
    "Due: Monday 2pm, Feburary 1, 2021, via Moodle\n",
    "\n",
    "Group 4: Christopher Klammt, Nils Krehl, Felix Hausberger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission guidelines\n",
    "\n",
    "- Solutions need to be uploaded as a single Jupyter notebook. You will find many provided codes in the notebook, your task is to fill in the missing cells.\n",
    "- For the written solution, use LaTeX in markdown inside the same notebook. Do *not* hand in a separate file for it.\n",
    "- Download the .zip file containing the dataset but do *not* upload it with your solution.\n",
    "- It is sufficient if one person per group uploads the solution to Moodle, but make sure that the names of\n",
    "all team members are given in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- 1.3: w2v.wv.most_similar(positive=[\"rachel\", \"man\"], negative=[\"women\"], topn=3)\n",
    "- 2.1: Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: F.R.I.E.N.D.S and  Word2Vec ( 3+ 2+ 3 = 8 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Friends is an American television sitcom, created by David Crane and Marta Kauffman. In this assignment we will use the transcripts from the show to train a Word2Vec model using the Gensim library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 1: Pre-processing \n",
    "We start by loading and cleaning the data. Download the dataset for this assignment and load the `friends_quotes.csv` using pandas. The dataset is from Kaggle (https://www.kaggle.com/ryanstonebraker/friends-transcript) and is created for building a classifier that could determine which friend from the Friend's TV Show would be most likely to say a quote. The column `quote` contains the line from the movie and `author` is the one who spoke it. Since these are the only two columns we need, we remove the rest and only keep these two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  \n",
    "import pandas as pd  \n",
    "from collections import defaultdict  \n",
    "import spacy \n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>quote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monica</td>\n",
       "      <td>There's nothing to tell! He's just some guy I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joey</td>\n",
       "      <td>C'mon, you're going out with the guy! There's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chandler</td>\n",
       "      <td>All right Joey, be nice. So does he have a hum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phoebe</td>\n",
       "      <td>Wait, does he eat chalk?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Phoebe</td>\n",
       "      <td>Just, 'cause, I don't want her to go through w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     author                                              quote\n",
       "0    Monica  There's nothing to tell! He's just some guy I ...\n",
       "1      Joey  C'mon, you're going out with the guy! There's ...\n",
       "2  Chandler  All right Joey, be nice. So does he have a hum...\n",
       "3    Phoebe                           Wait, does he eat chalk?\n",
       "4    Phoebe  Just, 'cause, I don't want her to go through w..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/friends_quotes.csv\") \n",
    "df.drop(['episode_number', 'episode_title', 'quote_order', 'season'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, there is no missing data, so we do not need to worry about that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author    0\n",
       "quote     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum() # check for missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use SpaCy similar to assignment 2 to pre-process the text, perform the following steps: \n",
    "- lowercase the words \n",
    "- remove the stopwords and single characters\n",
    "- use regex to remove non-alphabetic characters (anything that is not a number or alphabet including punctuations), in other words only keep \"a\" to \"z\" and digits. \n",
    "- remove lines that have less than 3 words, since they cannot contribute much to the training process.\n",
    "\n",
    "Please do not add additional steps on your own or additional cleaning, we want to create comparable results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/d070497/.local/share/virtualenvs/HateSpeechDetection-KN2PSeen/lib/python3.7/site-packages/ipykernel_launcher.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>quote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monica</td>\n",
       "      <td>tell guy work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joey</td>\n",
       "      <td>mon going guy gotta wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chandler</td>\n",
       "      <td>right joey nice hump hump hairpiece</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phoebe</td>\n",
       "      <td>wait eat chalk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Phoebe</td>\n",
       "      <td>cause don want went carl oh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     author                                quote\n",
       "0    Monica                        tell guy work\n",
       "1      Joey            mon going guy gotta wrong\n",
       "2  Chandler  right joey nice hump hump hairpiece\n",
       "3    Phoebe                       wait eat chalk\n",
       "4    Phoebe          cause don want went carl oh"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\",\"ner\"])\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS # only use these stop words, do not add your own! \n",
    "\n",
    "# Lowercase the words\n",
    "df[\"quote\"] = df[\"quote\"].str.lower()\n",
    "\n",
    "# Use regex to remove non-alphabetic characters\n",
    "df[\"quote\"] = df[\"quote\"].str.replace('[^a-zA-Z0-9 ]', ' ')\n",
    "\n",
    "# Remove the stopwords and single characters\n",
    "def remove_stopwords_and_single_characters(words):\n",
    "    return [word for word in words if not word in stopwords and len(word) > 1]\n",
    "df[\"quote\"] = df[\"quote\"].apply(lambda quote: \" \".join(remove_stopwords_and_single_characters(quote.split())))\n",
    "\n",
    "# Remove lines that have less than 3 words                               \n",
    "df.drop(df[df['quote'].apply(lambda sentence: len(sentence.split()) < 3)].index, inplace=True)               \n",
    "\n",
    "# To save all the lines                                 \n",
    "quotes = df[\"quote\"].tolist()\n",
    "        \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to build the vocabulary of the words and word combinations we want to learn representations from. We choose a subset of the most frequent words and bigrams to represent our corpus.\n",
    "- use the Gensim Phrases package to automatically detect common phrases (bigrams) from a list of lines from the previous step (`min_count=10`). Now words like New_York will be considered as one entity and character names like joey_tribbiani will be recognized.\n",
    "- create a list of words/bigrams with their frequencies and choose the top 15.000 words for the vocabulary, to keep the computation time-limited and choose the most important words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:36:53: collecting all words and their counts\n",
      "INFO - 18:36:53: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 18:36:53: PROGRESS: at sentence #10000, processed 72648 words and 54395 word types\n",
      "INFO - 18:36:54: PROGRESS: at sentence #20000, processed 146557 words and 95968 word types\n",
      "INFO - 18:36:54: PROGRESS: at sentence #30000, processed 219682 words and 132492 word types\n",
      "INFO - 18:36:54: collected 159721 word types from a corpus of 273450 words (unigram + bigrams) and 37477 sentences\n",
      "INFO - 18:36:54: using 159721 counts as vocab in Phrases<0 vocab, min_count=10, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 18:36:54: source_vocab length 159721\n",
      "INFO - 18:36:55: Phraser built with 199 phrasegrams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['know', 'oh', 'okay', 'don', 'yeah', 'right', 'like', 'hey', 'gonna', 'ross']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "sent = [quote.split() for quote in quotes] # list of all words\n",
    "phrases = Phraser(Phrases(sent, min_count=10)) # define the phraser for bi-gram creation\n",
    "new_lines = phrases[sent] #transform the lines\n",
    "\n",
    "assert 'new_york' in new_lines[969]\n",
    "assert 'joey_tribbiani' in new_lines[934]\n",
    "\n",
    "### find the top words for the vocabulary###\n",
    "flatten = lambda t: [item for sublist in t for item in sublist]\n",
    "vocab = [word for word, count in Counter(flatten(new_lines)).most_common(15000)] # top words\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 2: Training The Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Gensim the implementation of Word2Vec to train a model on the scripts. The training can be divided into 3 stages:\n",
    "\n",
    "\n",
    "1) Set up your model with parameters, define your parameters in such a way that the following conditions are satisfied:\n",
    " - ignores all words that have a total absolute frequency less than 2.\n",
    " - dimensions of the embeddings: 100 \n",
    " - initial learning rate of 0.03 \n",
    " - 20 negative samples \n",
    " - window size 3 \n",
    " - learning rate in the training will decrease as you apply more and more updates. Most of the time when starting with gradient descent the initial steps can be larger, and as we get close to the local minima it is best to use smaller steps to avoid jumping over the local minima. This adjustment is done internally using a learning rate scheduler. Make sure that the smallest learning rate does not go below 0.0001.\n",
    " - set the threshold for configuring which higher-frequency words are randomly down-sampled to 6e-5. This parameter forces the sampling to choose the very frequent words less often in the sampling.\n",
    " - set the hashfunction of the word2vec to the given function.\n",
    " - train on a single worker to make sure you get the same result as ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "def hash(astring):\n",
    "    return ord(astring[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(min_count=2, \n",
    "               size=100, \n",
    "               alpha=0.03, \n",
    "               negative=20, \n",
    "               window=3, \n",
    "               min_alpha=0.0001, \n",
    "               sample=6e-5, \n",
    "               hashfxn=hash, \n",
    "               workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) before training, Word2Vec requires us to build the vocabulary table by filtering out the unique words and doing some basic counts on them.\n",
    "If you look at the logs you can see the effect of `min_count` and `sample` on the word corpus. Use the `build_vocab` function to process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:24:03: collecting all words and their counts\n",
      "INFO - 16:24:03: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 16:24:03: PROGRESS: at sentence #10000, processed 70673 words, keeping 8526 word types\n",
      "INFO - 16:24:03: PROGRESS: at sentence #20000, processed 142473 words, keeping 11622 word types\n",
      "INFO - 16:24:03: PROGRESS: at sentence #30000, processed 213399 words, keeping 13830 word types\n",
      "INFO - 16:24:04: collected 15749 word types from a corpus of 265793 raw words and 37477 sentences\n",
      "INFO - 16:24:04: Loading a fresh vocabulary\n",
      "INFO - 16:24:04: effective_min_count=2 retains 9237 unique words (58% of original 15749, drops 6512)\n",
      "INFO - 16:24:04: effective_min_count=2 leaves 259281 word corpus (97% of original 265793, drops 6512)\n",
      "INFO - 16:24:04: deleting the raw counts dictionary of 15749 items\n",
      "INFO - 16:24:04: sample=6e-05 downsamples 952 most-common words\n",
      "INFO - 16:24:04: downsampling leaves estimated 120272 word corpus (46.4% of prior 259281)\n",
      "INFO - 16:24:04: estimated required memory for 9237 words and 100 dimensions: 12008100 bytes\n",
      "INFO - 16:24:04: resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "w2v.build_vocab(new_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Finally, we get to train the model. Train the model for 100 epochs. This will take a while. As we do not plan to train the model any further, we call `init_sims()`, which will make the model much more memory-efficient by precomputing L2-norms of word weight vectors for further analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:29:19: training model with 1 workers on 9237 vocabulary and 100 features, using sg=0 hs=0 sample=6e-05 negative=20 window=3\n",
      "INFO - 16:29:20: EPOCH 1 - PROGRESS: at 97.59% examples, 115549 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:29:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:20: EPOCH - 1 : training on 265793 raw words (120327 effective words) took 1.0s, 116730 effective words/s\n",
      "INFO - 16:29:21: EPOCH 2 - PROGRESS: at 97.59% examples, 117397 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:29:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:21: EPOCH - 2 : training on 265793 raw words (120097 effective words) took 1.0s, 118811 effective words/s\n",
      "INFO - 16:29:22: EPOCH 3 - PROGRESS: at 93.83% examples, 112223 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:22: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:22: EPOCH - 3 : training on 265793 raw words (120153 effective words) took 1.1s, 112248 effective words/s\n",
      "INFO - 16:29:23: EPOCH 4 - PROGRESS: at 93.83% examples, 112502 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:23: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:23: EPOCH - 4 : training on 265793 raw words (120330 effective words) took 1.0s, 116167 effective words/s\n",
      "INFO - 16:29:24: EPOCH 5 - PROGRESS: at 97.59% examples, 116305 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:29:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:24: EPOCH - 5 : training on 265793 raw words (120162 effective words) took 1.0s, 117734 effective words/s\n",
      "INFO - 16:29:25: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:25: EPOCH - 6 : training on 265793 raw words (120336 effective words) took 1.0s, 121504 effective words/s\n",
      "INFO - 16:29:26: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:26: EPOCH - 7 : training on 265793 raw words (120525 effective words) took 1.0s, 123643 effective words/s\n",
      "INFO - 16:29:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:27: EPOCH - 8 : training on 265793 raw words (120437 effective words) took 1.0s, 123097 effective words/s\n",
      "INFO - 16:29:28: EPOCH 9 - PROGRESS: at 93.83% examples, 112318 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:29:28: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:28: EPOCH - 9 : training on 265793 raw words (120223 effective words) took 1.0s, 114559 effective words/s\n",
      "INFO - 16:29:29: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:29: EPOCH - 10 : training on 265793 raw words (120316 effective words) took 1.0s, 121972 effective words/s\n",
      "INFO - 16:29:30: EPOCH 11 - PROGRESS: at 97.59% examples, 116439 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:29:30: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:30: EPOCH - 11 : training on 265793 raw words (120450 effective words) took 1.0s, 117855 effective words/s\n",
      "INFO - 16:29:31: EPOCH 12 - PROGRESS: at 86.11% examples, 100773 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:29:31: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:31: EPOCH - 12 : training on 265793 raw words (120260 effective words) took 1.1s, 104940 effective words/s\n",
      "INFO - 16:29:33: EPOCH 13 - PROGRESS: at 93.83% examples, 108273 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:33: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:33: EPOCH - 13 : training on 265793 raw words (120271 effective words) took 1.1s, 111939 effective words/s\n",
      "INFO - 16:29:34: EPOCH 14 - PROGRESS: at 89.89% examples, 107540 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:34: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:34: EPOCH - 14 : training on 265793 raw words (120413 effective words) took 1.1s, 110102 effective words/s\n",
      "INFO - 16:29:35: EPOCH 15 - PROGRESS: at 89.89% examples, 105061 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:29:35: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:35: EPOCH - 15 : training on 265793 raw words (119994 effective words) took 1.1s, 107705 effective words/s\n",
      "INFO - 16:29:36: EPOCH 16 - PROGRESS: at 93.83% examples, 110070 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:36: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:36: EPOCH - 16 : training on 265793 raw words (120293 effective words) took 1.1s, 113860 effective words/s\n",
      "INFO - 16:29:37: EPOCH 17 - PROGRESS: at 93.83% examples, 109271 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:37: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:37: EPOCH - 17 : training on 265793 raw words (120234 effective words) took 1.1s, 113058 effective words/s\n",
      "INFO - 16:29:38: EPOCH 18 - PROGRESS: at 97.59% examples, 117138 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:29:38: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:38: EPOCH - 18 : training on 265793 raw words (120443 effective words) took 1.0s, 118380 effective words/s\n",
      "INFO - 16:29:39: EPOCH 19 - PROGRESS: at 93.83% examples, 112228 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:39: EPOCH - 19 : training on 265793 raw words (120014 effective words) took 1.0s, 116064 effective words/s\n",
      "INFO - 16:29:40: EPOCH 20 - PROGRESS: at 89.89% examples, 105847 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:40: EPOCH - 20 : training on 265793 raw words (120083 effective words) took 1.1s, 110179 effective words/s\n",
      "INFO - 16:29:41: EPOCH 21 - PROGRESS: at 93.83% examples, 110464 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:41: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:41: EPOCH - 21 : training on 265793 raw words (120477 effective words) took 1.1s, 113762 effective words/s\n",
      "INFO - 16:29:42: EPOCH 22 - PROGRESS: at 89.89% examples, 104506 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:42: EPOCH - 22 : training on 265793 raw words (120153 effective words) took 1.1s, 106448 effective words/s\n",
      "INFO - 16:29:43: EPOCH 23 - PROGRESS: at 89.89% examples, 104058 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:43: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:43: EPOCH - 23 : training on 265793 raw words (120387 effective words) took 1.1s, 106038 effective words/s\n",
      "INFO - 16:29:44: EPOCH 24 - PROGRESS: at 89.89% examples, 105301 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:45: EPOCH - 24 : training on 265793 raw words (120413 effective words) took 1.1s, 109813 effective words/s\n",
      "INFO - 16:29:46: EPOCH 25 - PROGRESS: at 89.89% examples, 107452 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:46: EPOCH - 25 : training on 265793 raw words (120255 effective words) took 1.1s, 111497 effective words/s\n",
      "INFO - 16:29:47: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:47: EPOCH - 26 : training on 265793 raw words (120238 effective words) took 1.0s, 120660 effective words/s\n",
      "INFO - 16:29:48: EPOCH 27 - PROGRESS: at 97.59% examples, 116861 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:29:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:48: EPOCH - 27 : training on 265793 raw words (120195 effective words) took 1.0s, 117989 effective words/s\n",
      "INFO - 16:29:49: EPOCH 28 - PROGRESS: at 93.83% examples, 111459 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:49: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:49: EPOCH - 28 : training on 265793 raw words (120293 effective words) took 1.1s, 114240 effective words/s\n",
      "INFO - 16:29:50: EPOCH 29 - PROGRESS: at 97.59% examples, 115925 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:29:50: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:50: EPOCH - 29 : training on 265793 raw words (120263 effective words) took 1.0s, 117227 effective words/s\n",
      "INFO - 16:29:51: worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:29:51: EPOCH - 30 : training on 265793 raw words (120032 effective words) took 1.0s, 122117 effective words/s\n",
      "INFO - 16:29:52: EPOCH 31 - PROGRESS: at 97.59% examples, 114437 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:29:52: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:52: EPOCH - 31 : training on 265793 raw words (120496 effective words) took 1.0s, 115849 effective words/s\n",
      "INFO - 16:29:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:53: EPOCH - 32 : training on 265793 raw words (120406 effective words) took 1.0s, 122700 effective words/s\n",
      "INFO - 16:29:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:54: EPOCH - 33 : training on 265793 raw words (120164 effective words) took 1.0s, 121322 effective words/s\n",
      "INFO - 16:29:55: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:55: EPOCH - 34 : training on 265793 raw words (120291 effective words) took 1.0s, 122546 effective words/s\n",
      "INFO - 16:29:56: EPOCH 35 - PROGRESS: at 97.59% examples, 115292 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:29:56: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:56: EPOCH - 35 : training on 265793 raw words (120224 effective words) took 1.0s, 116559 effective words/s\n",
      "INFO - 16:29:57: EPOCH 36 - PROGRESS: at 89.89% examples, 107586 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:57: EPOCH - 36 : training on 265793 raw words (120656 effective words) took 1.1s, 111581 effective words/s\n",
      "INFO - 16:29:58: EPOCH 37 - PROGRESS: at 93.83% examples, 110993 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:58: EPOCH - 37 : training on 265793 raw words (119885 effective words) took 1.0s, 114237 effective words/s\n",
      "INFO - 16:29:59: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:59: EPOCH - 38 : training on 265793 raw words (120573 effective words) took 1.0s, 126412 effective words/s\n",
      "INFO - 16:30:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:00: EPOCH - 39 : training on 265793 raw words (120255 effective words) took 1.0s, 125967 effective words/s\n",
      "INFO - 16:30:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:01: EPOCH - 40 : training on 265793 raw words (120284 effective words) took 1.0s, 125711 effective words/s\n",
      "INFO - 16:30:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:02: EPOCH - 41 : training on 265793 raw words (119996 effective words) took 1.0s, 122777 effective words/s\n",
      "INFO - 16:30:03: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:03: EPOCH - 42 : training on 265793 raw words (119895 effective words) took 1.0s, 122257 effective words/s\n",
      "INFO - 16:30:04: EPOCH 43 - PROGRESS: at 97.59% examples, 114796 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:30:04: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:04: EPOCH - 43 : training on 265793 raw words (120267 effective words) took 1.0s, 116194 effective words/s\n",
      "INFO - 16:30:05: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:05: EPOCH - 44 : training on 265793 raw words (120405 effective words) took 1.0s, 123630 effective words/s\n",
      "INFO - 16:30:06: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:06: EPOCH - 45 : training on 265793 raw words (120203 effective words) took 1.0s, 123672 effective words/s\n",
      "INFO - 16:30:07: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:07: EPOCH - 46 : training on 265793 raw words (120630 effective words) took 1.0s, 122892 effective words/s\n",
      "INFO - 16:30:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:08: EPOCH - 47 : training on 265793 raw words (120549 effective words) took 1.0s, 123720 effective words/s\n",
      "INFO - 16:30:09: EPOCH 48 - PROGRESS: at 97.59% examples, 114527 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:30:09: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:09: EPOCH - 48 : training on 265793 raw words (120385 effective words) took 1.0s, 115831 effective words/s\n",
      "INFO - 16:30:10: EPOCH 49 - PROGRESS: at 82.60% examples, 99037 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:10: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:10: EPOCH - 49 : training on 265793 raw words (120376 effective words) took 1.2s, 102568 effective words/s\n",
      "INFO - 16:30:11: EPOCH 50 - PROGRESS: at 93.83% examples, 109573 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:11: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:11: EPOCH - 50 : training on 265793 raw words (119901 effective words) took 1.1s, 108184 effective words/s\n",
      "INFO - 16:30:12: EPOCH 51 - PROGRESS: at 97.59% examples, 116177 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:30:12: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:12: EPOCH - 51 : training on 265793 raw words (120198 effective words) took 1.0s, 117560 effective words/s\n",
      "INFO - 16:30:13: EPOCH 52 - PROGRESS: at 89.89% examples, 107449 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:13: EPOCH - 52 : training on 265793 raw words (120381 effective words) took 1.1s, 109575 effective words/s\n",
      "INFO - 16:30:14: EPOCH 53 - PROGRESS: at 89.89% examples, 107055 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:14: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:14: EPOCH - 53 : training on 265793 raw words (119933 effective words) took 1.1s, 110446 effective words/s\n",
      "INFO - 16:30:15: EPOCH 54 - PROGRESS: at 97.59% examples, 113570 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:30:15: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:15: EPOCH - 54 : training on 265793 raw words (120287 effective words) took 1.0s, 114828 effective words/s\n",
      "INFO - 16:30:17: EPOCH 55 - PROGRESS: at 86.11% examples, 95459 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:17: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:17: EPOCH - 55 : training on 265793 raw words (120552 effective words) took 1.3s, 94453 effective words/s\n",
      "INFO - 16:30:18: EPOCH 56 - PROGRESS: at 89.89% examples, 105822 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:18: EPOCH - 56 : training on 265793 raw words (120261 effective words) took 1.1s, 110090 effective words/s\n",
      "INFO - 16:30:19: EPOCH 57 - PROGRESS: at 97.59% examples, 116250 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:30:19: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:19: EPOCH - 57 : training on 265793 raw words (120676 effective words) took 1.0s, 117651 effective words/s\n",
      "INFO - 16:30:20: EPOCH 58 - PROGRESS: at 86.11% examples, 100444 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:20: EPOCH - 58 : training on 265793 raw words (120023 effective words) took 1.1s, 105132 effective words/s\n",
      "INFO - 16:30:21: EPOCH 59 - PROGRESS: at 86.11% examples, 103598 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:21: EPOCH - 59 : training on 265793 raw words (120456 effective words) took 1.1s, 107629 effective words/s\n",
      "INFO - 16:30:22: EPOCH 60 - PROGRESS: at 82.60% examples, 96737 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:22: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:22: EPOCH - 60 : training on 265793 raw words (120414 effective words) took 1.2s, 101691 effective words/s\n",
      "INFO - 16:30:23: EPOCH 61 - PROGRESS: at 93.83% examples, 108916 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:30:23: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:23: EPOCH - 61 : training on 265793 raw words (120417 effective words) took 1.1s, 110126 effective words/s\n",
      "INFO - 16:30:25: EPOCH 62 - PROGRESS: at 86.11% examples, 101887 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:25: worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:30:25: EPOCH - 62 : training on 265793 raw words (120311 effective words) took 1.2s, 103373 effective words/s\n",
      "INFO - 16:30:26: EPOCH 63 - PROGRESS: at 89.89% examples, 105699 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:30:26: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:26: EPOCH - 63 : training on 265793 raw words (120326 effective words) took 1.1s, 108359 effective words/s\n",
      "INFO - 16:30:27: EPOCH 64 - PROGRESS: at 93.83% examples, 110820 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:27: EPOCH - 64 : training on 265793 raw words (120249 effective words) took 1.1s, 114448 effective words/s\n",
      "INFO - 16:30:28: EPOCH 65 - PROGRESS: at 71.07% examples, 84907 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:28: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:28: EPOCH - 65 : training on 265793 raw words (120400 effective words) took 1.3s, 92696 effective words/s\n",
      "INFO - 16:30:29: EPOCH 66 - PROGRESS: at 97.59% examples, 114239 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:30:29: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:29: EPOCH - 66 : training on 265793 raw words (120329 effective words) took 1.0s, 115644 effective words/s\n",
      "INFO - 16:30:30: EPOCH 67 - PROGRESS: at 82.60% examples, 94616 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:30: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:30: EPOCH - 67 : training on 265793 raw words (120090 effective words) took 1.2s, 98034 effective words/s\n",
      "INFO - 16:30:32: EPOCH 68 - PROGRESS: at 89.89% examples, 103104 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:30:32: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:32: EPOCH - 68 : training on 265793 raw words (120085 effective words) took 1.1s, 105932 effective words/s\n",
      "INFO - 16:30:33: EPOCH 69 - PROGRESS: at 86.11% examples, 101475 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:33: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:33: EPOCH - 69 : training on 265793 raw words (120376 effective words) took 1.2s, 102805 effective words/s\n",
      "INFO - 16:30:34: EPOCH 70 - PROGRESS: at 75.06% examples, 88541 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:34: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:34: EPOCH - 70 : training on 265793 raw words (120698 effective words) took 1.4s, 89266 effective words/s\n",
      "INFO - 16:30:35: EPOCH 71 - PROGRESS: at 86.11% examples, 100678 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:30:35: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:35: EPOCH - 71 : training on 265793 raw words (120068 effective words) took 1.3s, 94257 effective words/s\n",
      "INFO - 16:30:36: EPOCH 72 - PROGRESS: at 86.11% examples, 100690 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:37: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:37: EPOCH - 72 : training on 265793 raw words (120081 effective words) took 1.1s, 105364 effective words/s\n",
      "INFO - 16:30:38: EPOCH 73 - PROGRESS: at 78.70% examples, 94140 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:38: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:38: EPOCH - 73 : training on 265793 raw words (120283 effective words) took 1.2s, 99916 effective words/s\n",
      "INFO - 16:30:39: EPOCH 74 - PROGRESS: at 93.83% examples, 110674 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:30:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:39: EPOCH - 74 : training on 265793 raw words (120356 effective words) took 1.1s, 112084 effective words/s\n",
      "INFO - 16:30:40: EPOCH 75 - PROGRESS: at 82.60% examples, 97700 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:40: EPOCH - 75 : training on 265793 raw words (119935 effective words) took 1.2s, 102407 effective words/s\n",
      "INFO - 16:30:41: EPOCH 76 - PROGRESS: at 93.83% examples, 108883 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:41: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:41: EPOCH - 76 : training on 265793 raw words (120317 effective words) took 1.1s, 110577 effective words/s\n",
      "INFO - 16:30:42: EPOCH 77 - PROGRESS: at 93.83% examples, 110844 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:42: EPOCH - 77 : training on 265793 raw words (120404 effective words) took 1.1s, 113154 effective words/s\n",
      "INFO - 16:30:43: EPOCH 78 - PROGRESS: at 86.11% examples, 100838 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:43: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:43: EPOCH - 78 : training on 265793 raw words (120176 effective words) took 1.1s, 106153 effective words/s\n",
      "INFO - 16:30:44: EPOCH 79 - PROGRESS: at 71.07% examples, 83601 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:45: EPOCH - 79 : training on 265793 raw words (120343 effective words) took 1.3s, 92447 effective words/s\n",
      "INFO - 16:30:46: EPOCH 80 - PROGRESS: at 93.83% examples, 112082 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:46: EPOCH - 80 : training on 265793 raw words (120089 effective words) took 1.0s, 115089 effective words/s\n",
      "INFO - 16:30:47: EPOCH 81 - PROGRESS: at 75.06% examples, 89756 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:47: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:47: EPOCH - 81 : training on 265793 raw words (120425 effective words) took 1.3s, 95344 effective words/s\n",
      "INFO - 16:30:48: EPOCH 82 - PROGRESS: at 89.89% examples, 103395 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:48: EPOCH - 82 : training on 265793 raw words (120028 effective words) took 1.1s, 106424 effective words/s\n",
      "INFO - 16:30:49: EPOCH 83 - PROGRESS: at 82.60% examples, 97408 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:49: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:49: EPOCH - 83 : training on 265793 raw words (120709 effective words) took 1.2s, 101012 effective words/s\n",
      "INFO - 16:30:50: EPOCH 84 - PROGRESS: at 89.89% examples, 105787 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:50: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:50: EPOCH - 84 : training on 265793 raw words (120407 effective words) took 1.1s, 108109 effective words/s\n",
      "INFO - 16:30:52: EPOCH 85 - PROGRESS: at 93.83% examples, 107848 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:30:52: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:52: EPOCH - 85 : training on 265793 raw words (120044 effective words) took 1.1s, 110038 effective words/s\n",
      "INFO - 16:30:53: EPOCH 86 - PROGRESS: at 93.83% examples, 108567 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:30:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:53: EPOCH - 86 : training on 265793 raw words (120171 effective words) took 1.1s, 110891 effective words/s\n",
      "INFO - 16:30:54: EPOCH 87 - PROGRESS: at 86.11% examples, 101369 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:54: EPOCH - 87 : training on 265793 raw words (120561 effective words) took 1.1s, 105566 effective words/s\n",
      "INFO - 16:30:55: EPOCH 88 - PROGRESS: at 78.70% examples, 92526 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:55: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:55: EPOCH - 88 : training on 265793 raw words (120083 effective words) took 1.2s, 99058 effective words/s\n",
      "INFO - 16:30:56: EPOCH 89 - PROGRESS: at 86.11% examples, 103493 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:56: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:56: EPOCH - 89 : training on 265793 raw words (120329 effective words) took 1.1s, 105592 effective words/s\n",
      "INFO - 16:30:57: EPOCH 90 - PROGRESS: at 82.60% examples, 96046 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:57: EPOCH - 90 : training on 265793 raw words (120117 effective words) took 1.3s, 93039 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:30:59: EPOCH 91 - PROGRESS: at 78.70% examples, 91334 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:59: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:59: EPOCH - 91 : training on 265793 raw words (120275 effective words) took 1.3s, 95718 effective words/s\n",
      "INFO - 16:31:00: EPOCH 92 - PROGRESS: at 86.11% examples, 103366 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:31:00: EPOCH - 92 : training on 265793 raw words (120284 effective words) took 1.1s, 108581 effective words/s\n",
      "INFO - 16:31:01: EPOCH 93 - PROGRESS: at 82.60% examples, 98453 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:31:01: EPOCH - 93 : training on 265793 raw words (120586 effective words) took 1.2s, 103467 effective words/s\n",
      "INFO - 16:31:02: EPOCH 94 - PROGRESS: at 89.89% examples, 106227 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:31:02: EPOCH - 94 : training on 265793 raw words (120014 effective words) took 1.2s, 100533 effective words/s\n",
      "INFO - 16:31:03: EPOCH 95 - PROGRESS: at 86.11% examples, 99393 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:31:03: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:31:03: EPOCH - 95 : training on 265793 raw words (120399 effective words) took 1.2s, 102627 effective words/s\n",
      "INFO - 16:31:04: EPOCH 96 - PROGRESS: at 97.59% examples, 114435 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:31:04: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:31:04: EPOCH - 96 : training on 265793 raw words (120226 effective words) took 1.0s, 115830 effective words/s\n",
      "INFO - 16:31:05: EPOCH 97 - PROGRESS: at 93.83% examples, 109597 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:31:06: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:31:06: EPOCH - 97 : training on 265793 raw words (120336 effective words) took 1.1s, 112440 effective words/s\n",
      "INFO - 16:31:07: EPOCH 98 - PROGRESS: at 93.83% examples, 111162 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:07: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:31:07: EPOCH - 98 : training on 265793 raw words (120202 effective words) took 1.0s, 114888 effective words/s\n",
      "INFO - 16:31:08: EPOCH 99 - PROGRESS: at 97.59% examples, 115394 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:31:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:31:08: EPOCH - 99 : training on 265793 raw words (120260 effective words) took 1.0s, 116911 effective words/s\n",
      "INFO - 16:31:09: EPOCH 100 - PROGRESS: at 97.59% examples, 114309 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:31:09: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:31:09: EPOCH - 100 : training on 265793 raw words (120264 effective words) took 1.0s, 115745 effective words/s\n",
      "INFO - 16:31:09: training on a 26579300 raw words (12028142 effective words) took 109.7s, 109655 effective words/s\n",
      "INFO - 16:31:09: precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "w2v.train(new_lines, total_examples=w2v.corpus_count, epochs=100)\n",
    "w2v.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 3: Exploring the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the lecture, word embeddings are suited for similarity and analogy tasks. Let's explore some of that with our dataset: \n",
    "\n",
    "We look for most similar words to the famous coffee shop where most of the episodes took place, namely `central_perk` and also one of the characters `joey`. If you have followed the exercise correctly until now, you should see that words like `laying` are similar to `central_perk` and the other main characters are also considered similar to `joey`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conan', 0.7044316530227661),\n",
       " ('sitting_couch', 0.6197500228881836),\n",
       " ('laying', 0.6157475709915161),\n",
       " ('recliner', 0.5554533004760742),\n",
       " ('puzzles', 0.5539599657058716),\n",
       " ('area', 0.5457411408424377),\n",
       " ('peek', 0.5379475355148315),\n",
       " ('movies', 0.536694347858429),\n",
       " ('dragon', 0.5342755913734436),\n",
       " ('whichever', 0.5340394973754883)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive=[\"central_perk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chandler', 0.8797323703765869),\n",
       " ('phoebe', 0.7789542078971863),\n",
       " ('ross', 0.7726583480834961),\n",
       " ('monica', 0.7707008123397827),\n",
       " ('turns', 0.763361930847168),\n",
       " ('rachel', 0.7601665258407593),\n",
       " ('goes', 0.7571671009063721),\n",
       " ('looks', 0.747425377368927),\n",
       " ('sees', 0.7224193811416626),\n",
       " ('quickly', 0.7035714983940125)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive=[\"joey\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the similarity of `mrs_green` to `rachel` (her mom) and `ross`  and `spaceship` (unrelated). The first one should have a high and the second low score. Finally look at the similarity of `smelly_cat` (a song from pheobe) and `song` the similarity should be high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6602634"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.similarity('mrs_green', 'rachel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.28437558"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.similarity('ross', 'spaceship')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5811626"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.similarity('smelly_cat', 'song')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also ask our model to give us the word that does not belong to a list of words. Let's see from the list of all 5 characters which one is the most dissimilar? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_names= ['joey', 'rachel', 'phoebe','monica','chandler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/d070497/.local/share/virtualenvs/HateSpeechDetection-KN2PSeen/lib/python3.7/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'joey'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.doesnt_match(character_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the analogies, which word is to `rachel` as `man` is to `women`? (print the top 3); you should get `chandler` and `monica` among the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('monica', 0.6889967918395996),\n",
       " ('ross', 0.6608829498291016),\n",
       " ('rest_gang', 0.6590893268585205)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive=[\"rachel\", \"man\"], negative=[\"women\"], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets use t-SNE to look at the distribution of our embeddings in the vector space for the character `joey`. Follow the instructions and fill in the blank in the `tsneplot` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsneplot(model, word):\n",
    "    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction for the top 10 most similar and dissimilar words\n",
    "    \"\"\"\n",
    "    embs = np.empty((0, 100), dtype='f') # to save all the embeddings\n",
    "    word_labels = [word]\n",
    "    color_list  = ['green']\n",
    "\n",
    "   \n",
    "    embs = np.append(embs, model.wv.__getitem__([word]), axis=0) # adds the vector of the query word\n",
    "    \n",
    "   \n",
    "    close_words = model.wv.most_similar([word]) # gets list of most similar words\n",
    "    all_sims = sorted(model.wv.most_similar([word], topn=model.corpus_count))[:10] # gets list of most dissimilar words (get the sorted list of all the words and their similarity and choose the bottom 10 )\n",
    "    print(all_sims)\n",
    "    \n",
    "    # adds the vector for each of the closest words to the array\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv.__getitem__([wrd_score[0]]) # get the vector\n",
    "        word_labels.append(wrd_score[0])\n",
    "        color_list.append('blue')\n",
    "        embs = np.append(embs, wrd_vector, axis=0)\n",
    "        \n",
    "    # adds the vector for each of the furthest words to the array\n",
    "    for wrd_score in all_sims:\n",
    "        wrd_vector = model.wv.__getitem__([wrd_score[0]]) # get the vector \n",
    "        word_labels.append(wrd_score[0])\n",
    "        color_list.append('red')\n",
    "        embs = np.append(embs, wrd_vector, axis=0)\n",
    "    \n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = TSNE(n_components=2, random_state=42, perplexity=15).fit_transform(embs) # with n_components=2, random_state=42, perplexity=15 \n",
    "    \n",
    "    # Sets everything up to plot\n",
    "    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n",
    "                       'y': [y for y in Y[:, 1]],\n",
    "                       'words': word_labels,\n",
    "                       'color': color_list})\n",
    "    \n",
    "    fig, _ = plt.subplots()\n",
    "    fig.set_size_inches(10, 10)\n",
    "    \n",
    "    # Basic plot\n",
    "    p1 = sns.regplot(data=df,\n",
    "                     x=\"x\",\n",
    "                     y=\"y\",\n",
    "                     fit_reg=False,\n",
    "                     marker=\"o\",\n",
    "                     scatter_kws={'s': 40,\n",
    "                                  'facecolors': df['color']\n",
    "                                 }\n",
    "                    )\n",
    "    \n",
    "    # adds annotations one by one with a loop\n",
    "    for line in range(0, df.shape[0]):\n",
    "         p1.text(df[\"x\"][line],\n",
    "                 df['y'][line],\n",
    "                 '  ' + df[\"words\"][line].title(),\n",
    "                 horizontalalignment='left',\n",
    "                 verticalalignment='bottom', size='medium',\n",
    "                 color=df['color'][line],\n",
    "                 weight='normal'\n",
    "                ).set_size(15)\n",
    "\n",
    "    \n",
    "    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n",
    "    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n",
    "            \n",
    "    plt.title('t-SNE visualization for {}'.format(word.title()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('00', -0.0701979249715805), ('000', -0.10169506072998047), ('007', 0.008701125159859657), ('02', 0.06537488102912903), ('038', 0.12504784762859344), ('10', -0.05131617188453674), ('100', -0.054360613226890564), ('1066', 0.08148254454135895), ('10th', -0.07627394795417786), ('11', -0.1710900366306305)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tsneplot(w2v, 'joey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Multi-class Classification ( 1+ 3 + 2 = 6 points)\n",
    "In this task, we aim to classify consumer finance complaints into 12 pre-defined classes. Note that this is not a multi-label task, and we assume that each new complaint is assigned to one and only one category. The data comes from https://www.data.gov/ (US governments open data) and contains complaints that are published after the company responds, confirming a commercial relationship with the consumer, or after 15 days, whatever comes first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 1: Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data to a pandas dataframe from `complaints.csv`; this is a rather large file of 206MB. Keep only the `Consumer complaint narrative` (input text) and `product`(labels). Remove the missing values, rename `Consumer complaint narrative` to `Narrative` for ease of use, and add a column encoding the product as an integer. This will represent your labels for classification and the mapping will be used later on. Create two dictionaries: one mapping the ids to products and one mapping products to their ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>narrative</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Checking or savings account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I was sold access to an event digitally, of wh...</td>\n",
       "      <td>Money transfer, virtual currency, or money ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Vehicle loan or lease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Checking or savings account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Checking or savings account</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           narrative  \\\n",
       "0                                                NaN   \n",
       "1  I was sold access to an event digitally, of wh...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                             product  \n",
       "0                        Checking or savings account  \n",
       "1  Money transfer, virtual currency, or money ser...  \n",
       "2                              Vehicle loan or lease  \n",
       "3                        Checking or savings account  \n",
       "4                        Checking or savings account  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "df = pd.read_csv(\"data/complaints.csv\")\n",
    "df = df[['Consumer complaint narrative','Product']] # keep the columns you need\n",
    "df.columns =  ['narrative', 'product'] # rename columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>narrative</th>\n",
       "      <th>product</th>\n",
       "      <th>product_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I was sold access to an event digitally, of wh...</td>\n",
       "      <td>Money transfer, virtual currency, or money ser...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I LEASED MY CAR ON XX/XX/XXXX AND MADE PAYMENT...</td>\n",
       "      <td>Vehicle loan or lease</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>My original lender was XXXX XXXX XXXX ; this l...</td>\n",
       "      <td>Student loan</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I was misled by XXXX XXXX  and my degree did n...</td>\n",
       "      <td>Student loan</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I purchased a vehicle 3 years ago. As of recen...</td>\n",
       "      <td>Vehicle loan or lease</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            narrative  \\\n",
       "1   I was sold access to an event digitally, of wh...   \n",
       "5   I LEASED MY CAR ON XX/XX/XXXX AND MADE PAYMENT...   \n",
       "6   My original lender was XXXX XXXX XXXX ; this l...   \n",
       "8   I was misled by XXXX XXXX  and my degree did n...   \n",
       "10  I purchased a vehicle 3 years ago. As of recen...   \n",
       "\n",
       "                                              product  product_id  \n",
       "1   Money transfer, virtual currency, or money ser...           0  \n",
       "5                               Vehicle loan or lease           1  \n",
       "6                                        Student loan           2  \n",
       "8                                        Student loan           2  \n",
       "10                              Vehicle loan or lease           1  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna() # remove missing\n",
    "df['product_id'], _ = pd.factorize(df['product']) # integer ids for product\n",
    "\n",
    "### create the dictionary ####\n",
    "id2product = pd.Series(df[\"product\"].values,index=df['product_id']).to_dict()\n",
    "product2id = pd.Series(df[\"product_id\"].values,index=df['product']).to_dict()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the class distribution of the products. It is always a good idea to look at the relative number of instances for each class before performing any classification task. Use the `plot` function from the dataframe to show the number of instances in each class in a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 720x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legend:\n",
      "{0: 'Money transfer, virtual currency, or money service', 1: 'Vehicle loan or lease', 2: 'Student loan', 3: 'Checking or savings account', 4: 'Credit card', 5: 'Payday loan, title loan, or personal loan', 6: 'Consumer Loan', 7: 'Payday loan', 8: 'Bank account or service', 9: 'Credit reporting', 10: 'Other financial service', 11: 'Money transfers', 12: 'Prepaid card', 13: 'Virtual currency'}\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "df.plot.hist(by=\"product\", title=\"Class Distribution\", legend=True, figsize=(10,8))\n",
    "plt.show()\n",
    "print(\"Legend:\")\n",
    "print(id2product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have done the exercise correctly you should observe a class imbalance with `credit reporting` having the most complaints. This can result in some difficulties for standard algorithms, making them biased towards the majority class and treating the minority classes as outliers and unimportant. One way to overcome this problem is by using **undersampling** or **oversampling**. However, this is beyond the scope of this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 2: Text Representation and Training the Classifier \n",
    "Before performing any classification we need to split our data into train and test sets. Use `sklearn` to save 20 percent of the data for the test and the rest for training. Make sure to input the index of the data frame to retrieve the indices of the test and train. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(df[\"narrative\"].tolist(), df[\"product_id\"].tolist(), df.index, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform any sort of classification task, we first need to convert our raw text into some vector representation. Let's use the `TfidfVectorizer` from `sklearn` to convert the `narrative` column into TF-IDF vectors. When transforming the text keep the following in mind:\n",
    "- use the logarithmic form for frequency\n",
    "- remove accents (ASCII) \n",
    "- lowercase all characters \n",
    "- remove `English` stop words \n",
    "- ignore terms that have a document frequency strictly less than 10\n",
    "- smooth IDF weights by adding one to document frequencies \n",
    "- output row should have unit L2 norm\n",
    "- set the encoding to `Latin-1`\n",
    "- extract both uni-grams and bi-grams \n",
    "- build a vocabulary that only considers the top 10.000 features\n",
    "Keep in mind that the vectorizer should be trained **only** on the training data, and the test data should be transformed using the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(###initlize the model ### \n",
    "X_train = ###transform text  ### \n",
    "X_train.shape # should be (132647, 10000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data transformation, we attain the features and labels, to train the classifier. In our case, we use **Naive Bayes Classifier**. \n",
    "- split the features and labels into training and test set (set the random state to 42) \n",
    "- use `MultinomialNB` from sklearn to classify the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf =### fit the model to the data ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 3: Model Evaluation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate our model using the held-out test data. We are going to look at the confusion matrix to show the performance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "y_pred = ### predict on the test set ### \n",
    "conf = ### create the confusion matrix ### \n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(conf, annot=True, fmt='d',\n",
    "            xticklabels=###products names from the dictionary ###,\n",
    "            yticklabels=### products names from the dictionary###)\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the predictions end up on the diagonal (predicted label = actual label). The diagonal shows the correct classified classes. However, there are several misclassifications, specifically `Checking or savings account` is often confused with `Bank account or service`. Let's take a look at why this happens. For this, we look at 5 misclassified examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual='Bank account or service'\n",
    "predicted= 'Checking or savings account'\n",
    "### print only the top 10 \n",
    "df###choose the ones that have an actual label of Bank account or service and the predicted label of Checking or savings account ###)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, some of the misclassified complaints are complaints that are not easy to distinguish. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Auto-Complete ( 2 + 5 + 4 = 11 Points ) \n",
    "Let's get even more practical! In this problem set, you will build your own auto-completion system that you see every day while using search engines.\n",
    "\n",
    "[google]: https://www.thedad.com/wp-content/uploads/2018/05/screen-shot-2018-05-12-at-2-01-56-pm.png \"google auto complete\"\n",
    "\n",
    "![google]\n",
    "By the end of this assignment, you will develop a simple prototype of such a system using n-gram language models. At the heart of the system is a language model that assigns the probability to a sequence of words. We take advantage of this probability calculation to predict the next word. \n",
    "\n",
    "The problem set contains 3 main parts:\n",
    "\n",
    "1. Load and preprocess data (tokenize and split into train and test)\n",
    "2. Develop n-gram based language models by estimating the conditional probability of the next word.\n",
    "3. Evaluate the model by computing the perplexity score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 1: Load and Preprocess Data \n",
    "We use a subset of English tweets to train our model. Run the cell below to load the data and observe a few lines of it. Notice that tweets are saved in a text file, where tweets are separated by `\\n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\",\"ner\"])\n",
    "\n",
    "\n",
    "with open(\"twitter.txt\", \"r\") as f:\n",
    "    data = f.read()\n",
    "print(\"First 500 characters of the data:\")\n",
    "display(data[0:500])\n",
    "print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to separate the tweets and split them into train and test set. Apply the following pre-processing steps:\n",
    "\n",
    "1. Split data into sentences using \"\\n\" as the delimiter and remove the leading and trailing spaces (drop empty sentences)\n",
    "2. Tokenize the sentences into words using SpaCy and lowercase them. (notice that we do not remove stop words or punctuations.) \n",
    "3. Divide the sentences into 80 percent training and 20 percent test set. No validation set is required. Although in a real-world application it is best to set aside part of the data for hyperparameter tuning.\n",
    "4. To limit the vocabulary and remove potential spelling mistakes, make a vocabulary of the words that appear at least 2 times. The rest of the words will be replaced by the `<unk>` symbol. This is a crucial step since if your model encounters a word that it never saw during training, it won't have an input word that helps determining the next word for suggestion. We use the `<unk>` word for **out of Vocabulary (OOV)** words. Keep in mind that we built the vocabulary on the training data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = #split\n",
    "sentences = #remove spaces and drop empty sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = []# list of list of the tokens in a sentence \n",
    "##Your Code###     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import Random\n",
    "Random(4).shuffle(tokenized_corpus)\n",
    "\n",
    "train = ##Your Code###     \n",
    "test = ##Your Code###     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "flatten_corpus = ### Flatten the train corpus ### \n",
    "word_counts = ### count the number of each token ### \n",
    "vocab = []\n",
    "\n",
    "### keep only the ones with frequency bigger than 2 ### \n",
    "print(len(vocab)) ### should be 16930 ### \n",
    "train_replaced = []\n",
    "test_replaced = []\n",
    "for sentence in train:\n",
    "    ### adjust the sentence to contain the word in the vocabulary and <unk> for the rest #### \n",
    "for sentence in test:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 2: N-gram Based Language Model: \n",
    "In this section, you will develop the n-grams language model. We assume that the probability of the next word depends only on the previous n-gram or previous n words. We compute this probability by counting the occurrences in the corpus.\n",
    "The conditional probability for the word at position 't' in the sentence, given that the words preceding it are $w_{t-1}, w_{t-2} \\cdots w_{t-n}$ can be estimated as follows:\n",
    "\n",
    "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n)}{C(w_{t-1}\\dots w_{t-n})}  $$\n",
    "\n",
    "The numerator is the number of times word 't' appears after the n-gram, and the denominator is the number of times the n-gram occurs in the corpus, where $C(\\cdots)$ is a count function. Later, we add k-smoothing to avoid errors when any counts are zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle the problem of probability estimation we divide the problem into 3 parts. In the following you will: \n",
    "1. Implement a function that computes the counts of n-grams for an arbitrary number n.\n",
    "2. Estimate the probability of a word given the prior n-words using the n-gram counts.\n",
    "3. Calculate probabilities for all possible words.\n",
    "4. Create a probability matrix. \n",
    "The steps are detailed below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by implementing a function that computes the counts of n-grams for an arbitrary number n.\n",
    "- Prepend necessary starting markers `<s>` to indicate the beginning of the sentence. In the case of a bi-gram model, you need to prepend two start tokens `<s><s>` to be able to predict the first word. \"hello world\"-> \"`<s><s>`hello world\".\n",
    "- Append an end token `<e>` so that the model can predict when to finish a sentence.\n",
    "- Create a dictionary to store all the n_gram counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams_counts(corpus, n):\n",
    "    \"\"\"\n",
    "    Count all n-grams in the corpus given the parameter n \n",
    "    \n",
    "    data: List of lists of words (your tokenized corpus)\n",
    "    n: n in the n-gram\n",
    "    \n",
    "    Returns: A dictionary that maps a tuple of n words to its frequency\n",
    "    \"\"\"\n",
    "    start_token='<s>'\n",
    "    end_token = '<e>'\n",
    "    n_grams = defaultdict(int)\n",
    "    for sentence in corpus: \n",
    "        sentence = ### add start and end token ###\n",
    "        # convert list to tuple so it can be used a the key in the dictionary \n",
    "        sentence = tuple(sentence)\n",
    "        \n",
    "        ###iterate over the n-grams in the sentence, you can use the range() function, and increament the counts in the\n",
    "        ## n_grams dictionary, where the key is the n_gram and the value is count \n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to estimate the probability of a word given the prior n words using the n-gram counts, based on the formula given at the beginning of this task. To deal with the problem of zero division we add k-smoothing. K-smoothing adds a positive constant $k$ to each numerator and $k \\times |vocabulary size|$ in the denominator. Below we will define a function that takes in a dictionary `n_gram_cnt`, where the key is the n-gram, and the value is the count of that n-gram, plus a dictionary for `plus_current_gram_cnt`, which you'll use to find the count for the previous n-gram plus the current word. Notice that these dictionaries are computed using the previous function `n_grams_counts`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(word, prev_n_gram, \n",
    "                         n_gram_cnts, n_plus1_gram_cnts, vocab_size):\n",
    "    \"\"\"\n",
    "    Estimate the probabilities of a next word using the n-gram counts with k-smoothing\n",
    "    word: next word\n",
    "    prev_n_gram: previous n gram\n",
    "    n_gram_cnts: dictionary of counts of n-grams\n",
    "    plus_current_gram_cnts: dictionary of counts of (n+1)-grams -> current word+ n-gram = (n+1)-gram\n",
    "    vocab_size: number of words in the vocabulary\n",
    "    \n",
    "    Returns: A probability\n",
    "    \"\"\"\n",
    "    k=1.0\n",
    "    prev_n_gram = tuple(prev_n_gram)\n",
    "    \n",
    "    prev_n_gram_cnt =  # get the previous n-gram count from the dictionary \n",
    "    denominator = # denominator with the previous n-gram count and k-smoothing\n",
    "    n_plus1_gram =  # add the current word to the n-gram \n",
    "    n_plus1_gram_cnt =  # get the current n-gram count using the dictionary\n",
    "    numerator = #calculate the numerator with k-smoothing\n",
    "    prob =\n",
    "    \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the functions we have defined to calculate probabilities for all possible words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilities(prev_n_gram, n_gram_cnts, plus_current_gram_cnts, vocab):\n",
    "    \"\"\"\n",
    "    Estimate the probabilities for all the words in the vocabulary given the previous n-gram \n",
    "    prev_n_gram: previous n-gram\n",
    "    n_gram_cnts: dictionary of counts of n-grams\n",
    "    plus_current_gram_cntsplus_current_gram_cnt: dictionary of counts of (n+1)-grams -> current word+ n-gram = (n+1)-gram\n",
    "    vocab: List of words\n",
    "    \n",
    "    Returns: A dictionary mapping from next words to the probability.\n",
    "    \"\"\"\n",
    "    prev_n_gram = tuple(prev_n_gram)\n",
    "    \n",
    "    vocab =  # add <e> <unk> to the vocabulary\n",
    "    vocabulary_size = #compute the size \n",
    "    \n",
    "    probabilities = {}\n",
    "    for word in vocab:\n",
    "        ### compute the probability \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the probability of the all possible words after the unigram \"the\"\n",
    "sentences = [['the', 'moon', 'and', 'stars', 'are','shining','bright'],\n",
    "             ['the', 'moon', 'is', 'shinnig','tonight'],\n",
    "             ['mars','and' ,'moon', 'are', 'plants'],\n",
    "             ['the' ,'moon', 'is','a', 'plant']]\n",
    "unique_words = list(set(sentences[0] + sentences[1] + sentences[2]+ sentences[3]))\n",
    "unigram_counts = n_grams_counts(sentences, 1)\n",
    "bigram_counts = n_grams_counts(sentences, 2)\n",
    "print(\"The word 'moon' should have the highest probability, if it is not the case, re-visit your previous functions.\")\n",
    "probabilities([\"the\"], unigram_counts, bigram_counts, unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub Task 3: Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we use the perplexity score to evaluate your model on the test set.\n",
    "the perplexity score of the test set on an n-gram model, is denoted as follows: \n",
    "\n",
    "$$ PP(W) =\\sqrt[N]{ \\prod_{t=n}^{N-1} \\frac{1}{P(w_t | w_{t-n} \\cdots w_{t-1})} } $$\n",
    "- where $N$ is the length of the sentence. ($N-1$ is used because in the code we start from the index 0).\n",
    "- $n$ is the number of words in the n-gram.\n",
    "\n",
    "Notice that we have already computed this probability. \n",
    "\n",
    "The higher the probabilities are, the lower the perplexity will be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(sentence, n_gram_cnts, plus_current_gram_cnts, vocab_size, k=1.0):\n",
    "    \"\"\"\n",
    "    Calculate perplexity for a list of sentences\n",
    "    sentence: List of strings\n",
    "    n_gram_cnts: dictionary of counts of n-grams\n",
    "    plus_current_gram_cnts: dictionary of counts of (n+1)-grams -> current word+ n-gram = (n+1)-gram\n",
    "    vocab_size: number of unique words in the vocabulary\n",
    "    k: positive smoothing constant\n",
    "    \n",
    "    Returns: Perplexity score for a single sentence \n",
    "    \"\"\"\n",
    "    \n",
    "    n =  # get the number 'n' in  n-gram  from n_gram_cnts  \n",
    "    \n",
    "    sentence =  # prepend <s> and append <e>\n",
    "    sentence = tuple(sentence)\n",
    "    N =# length of sentence \n",
    "    \n",
    "   \n",
    "    product_pi = 1.0 \n",
    "    \n",
    "    ### Compute the product of probabilites ###\n",
    "    \n",
    "    for t in range(n, N): \n",
    "        n_gram =# get the n-gram before the predicted word (n-gram before t )\n",
    "        word =  # get the word to be predicted (position t)\n",
    "        prob = probability(\n",
    "        product_pi *= # Update the product of the probabilities\n",
    "    \n",
    "    perplexity = product_pi**(1/float(N)) # Take the Nth root of the product\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function to find the perplexity of a bi-gram model on the first instance of training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counts = ### your code ###\n",
    "trigram_counts = ### your code ###\n",
    "\n",
    "perplexity_train = perplexity(train_replaced[0],bigram_counts, trigram_counts,len(vocab))\n",
    "print(f\"Perplexity for first train sample: {perplexity_train:.4f}\")\n",
    "\n",
    "perplexity_test = perplexity(test_replaced[0],bigram_counts, trigram_counts,len(vocab))\n",
    "print(f\"Perplexity for test sample: {perplexity_test:.4f}\")# the preprexity for the train sample should be much lower "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's use the model we created to generate an auto-complete system that makes suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_a_word(up_to_here, n_gram_cnts, plus_current_gram_cnts, vocab , start_with=None):\n",
    "    \"\"\"\n",
    "    Get suggestion for the next word\n",
    "    up_to_here: the sentence so far, must have length > n \n",
    "    n_gram_cnts: dictionary of counts of n-grams\n",
    "    plus_current_gram_cnts: dictionary of counts of (n+1)-grams -> current word+ n-gram = (n+1)-gram\n",
    "    vocab: List of words\n",
    "    start_with: If not None, specifies the first few letters of the next word\n",
    "        \n",
    "    Returns: (most likely next word,  probability) \n",
    "    \"\"\"    \n",
    "    n = len(list(n_gram_cnts.keys())[0]) # get the number 'n' in  n-gram  from n_gram_cnts     \n",
    "    previous_n_gram = previous_tokens[-n:] # get the last 'n' words as the previous n-gram from the input sentence\n",
    "\n",
    "    \n",
    "    probabs = # Estimate the probabilities for each word in the vocabulary\n",
    "    \n",
    "    probabs = \n",
    "    ### sort the probability for higher to lower and return the highest probability word,probability tuple\n",
    "    #if start_with is specified then return the highest probability word that starts with that specific character \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your model based on the bi-gram model created on the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens=['i','like']\n",
    "start_with='g'\n",
    "suggestion = suggest_a_word(previous_tokens, bigram_counts,trigram_counts, list(vocab),start_with=start_with)\n",
    "suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens=['i','like','to']\n",
    "start_with=None\n",
    "suggestion = suggest_a_word(previous_tokens, bigram_counts,trigram_counts, list(vocab),start_with=start_with)\n",
    "suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens=[\"hello\", \"my\", \"name\", \"is\"]\n",
    "start_with=None\n",
    "suggestion = suggest_a_word(previous_tokens, bigram_counts,trigram_counts, list(vocab),start_with=start_with)\n",
    "suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens=[\"hello\", \"my\", \"name\", \"is\"]\n",
    "start_with='sa'\n",
    "suggestion = suggest_a_word(previous_tokens, bigram_counts,trigram_counts, list(vocab),start_with=start_with)\n",
    "suggestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Theoretical Questions ( 1 +1 +1 +1 + 1 = 5 Points) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please answer the following questions in the notebook cells using markdown, try to be percise and short. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 1: What is the goal of Word2Veec? / What is a word embedding?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<<< your answer >>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 2: For gradient descent, what advantage has a decaying learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<<< your answer >>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 3: Why is it easier to maximize the log likelihood instead of the \"normal\" likelihood?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<<< your answer >>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 4: Word2Vec trains a complete neural network. How do we extract word embeddings from this network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<<< your answer >>>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 5: What advantages has fastText over Word2Vec? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<<< your answer >>>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
